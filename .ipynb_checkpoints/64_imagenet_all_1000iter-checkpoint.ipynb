{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cnn4_triplet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7505/3753837643.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mTriplets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcnn4_triplet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cnn4_triplet'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "#import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas.core.common import flatten\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import learn2learn as l2l\n",
    "from losses import *\n",
    "from Triplets import *\n",
    "from cnn4_triplet import *\n",
    "\n",
    "import datasets\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TripletFSCIFAR100' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6928/1124797776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m Triplet_Model_Parameter = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m\"CIFARFS\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTripletFSCIFAR100\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"root\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"~/data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"download\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"transform\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_pool\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embedding_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"margin\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lambda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"CUB\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTripletCUB\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"root\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"download\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"transform\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_pool\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embedding_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"margin\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lambda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"FLOWERS\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTripletFlowers\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"root\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"~/data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"download\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"transform\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_pool\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embedding_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"margin\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lambda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"MINIIMAGENET\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTripletMiniImageNet\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"root\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"download\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"transform\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_pool\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embedding_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"margin\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"lambda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TripletFSCIFAR100' is not defined"
     ]
    }
   ],
   "source": [
    "Triplet_Model_Parameter = {\n",
    "    \"CIFARFS\" : {\"data\" : TripletFSCIFAR100 , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":256,\"margin\":1.0,\"lambda\":1} ,\n",
    "    \"CUB\" : {\"data\" : TripletCUB , \"root\" : \"./data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "    \"FLOWERS\" : {\"data\" : TripletFlowers , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "    \"MINIIMAGENET\" : {\"data\" : TripletMiniImageNet , \"root\" : \"data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":32, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":800,\"margin\":1.0,\"lambda\":1},\n",
    "    \"OMNIGLOT\" : {\"data\" : TripletOmniglot , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor(),transforms.Resize((28,28))]), \"hidden_size\":64, \"layers\":4, \"channels\":1, \"max_pool\":False, \"embedding_size\":256,\"margin\":1.0,\"lambda\":1},\n",
    "    \"MINIIMAGENET_64\" : {\"data\" : TripletMiniImageNet , \"root\" : \"data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "    \"MINIIMAGENET_RS\" : {\"data\" : TripletMiniImageNet , \"root\" : \"data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":128, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":3200,\"margin\":1.0,\"lambda\":1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets, shots):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    # return (predictions == targets).sum().float() / targets.size(0)\n",
    "\n",
    "    if shots == 1:\n",
    "        mask = np.array([True, False, False, False, False, False, False, False, True, True, True, True])\n",
    "    else: # shots==5\n",
    "        mask = np.array(\n",
    "        [True, False, False, False, False, False, False, False, False, False, False, False,\tFalse, False, False, False, False, False, False, False, \n",
    "        True, False, False, False, False, True, False, False, False, False, True, False, False, False, False, True, False, False, False, False,\n",
    "        True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True ,True, True])\n",
    "    return (predictions[mask] == targets[mask]).sum().float() / targets[mask].size(0)\n",
    "\n",
    "\n",
    "def fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    adaptation_data, adaptation_labels, evaluation_data, evaluation_labels = batch\n",
    "    adaptation_data[0]= adaptation_data[0].to(device)\n",
    "    adaptation_data[1]= adaptation_data[1].to(device)\n",
    "    adaptation_data[2]= adaptation_data[2].to(device)\n",
    "\n",
    "    adaptation_labels= adaptation_labels.to(device)\n",
    "\n",
    "    evaluation_data[0]= evaluation_data[0].to(device)\n",
    "    evaluation_data[1]= evaluation_data[1].to(device)\n",
    "    evaluation_data[2]= evaluation_data[2].to(device)\n",
    "\n",
    "    evaluation_labels= evaluation_labels.to(device)\n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        out= learner(adaptation_data[0], adaptation_data[1], adaptation_data[2] )\n",
    "        train_error = loss(out[0],out[1],out[2], torch.vstack([out[3],out[4],out[5]]), torch.hstack([adaptation_labels[0],adaptation_labels[1],adaptation_labels[2]]))\n",
    "        learner.adapt(train_error, allow_unused=True)\n",
    "\n",
    "    # Evaluate the adapted model\n",
    "    predictions = learner(evaluation_data[0], evaluation_data[1], evaluation_data[2])\n",
    "    valid_error= loss(predictions[0],predictions[1],predictions[2], torch.vstack([predictions[3],predictions[4],predictions[5]]), torch.hstack([evaluation_labels[0],evaluation_labels[1],evaluation_labels[2]]))\n",
    "    valid_accuracy = accuracy(torch.vstack([predictions[3],predictions[4],predictions[5]]), torch.hstack([evaluation_labels[0],evaluation_labels[1],evaluation_labels[2]]), shots)\n",
    "    return valid_error, valid_accuracy\n",
    "\n",
    "def fast_adapt_image_retrieval(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    adaptation_data, adaptation_labels, evaluation_data, evaluation_labels = batch\n",
    "    adaptation_data[0]= adaptation_data[0].to(device)\n",
    "    adaptation_data[1]= adaptation_data[1].to(device)\n",
    "    adaptation_data[2]= adaptation_data[2].to(device)\n",
    "\n",
    "    adaptation_labels= adaptation_labels.to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        out= learner(adaptation_data[0], adaptation_data[1], adaptation_data[2] )\n",
    "        train_error = loss(out[0],out[1],out[2], torch.vstack([out[3],out[4],out[5]]), torch.hstack([adaptation_labels[0],adaptation_labels[1],adaptation_labels[2]]))\n",
    "        learner.adapt(train_error, allow_unused=True)\n",
    "\n",
    "   \n",
    "    return train_error, train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways=5  # in our triplet implementation, number of distinct classes is 5\n",
    "shots=1\n",
    "meta_lr=0.001  # as in MAML\n",
    "fast_lr=0.01  # as in MAML\n",
    "meta_batch_size=4  # Maml Omniglot:32; miniImageNet: 4\n",
    "adaptation_steps=15\n",
    "test_adaptation_steps=10\n",
    "num_iterations=60000  # as in MAML\n",
    "cuda=True\n",
    "seed=42\n",
    "num_test_episodes=600\n",
    "selected_model=\"MINIIMAGENET_64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6928/2058988366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seed' is not defined"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device('cuda')    \n",
    "\n",
    "triplet_imagenet_dataset = Triplet_Model_Parameter[selected_model][\"data\"](root = Triplet_Model_Parameter[selected_model][\"root\"], download = Triplet_Model_Parameter[selected_model][\"download\"], transform = Triplet_Model_Parameter[selected_model][\"transform\"])\n",
    "\n",
    "# Create model using saved parameters:\n",
    "model = TripletCNN4(output_size= ways, hidden_size=Triplet_Model_Parameter[selected_model][\"hidden_size\"], layers=Triplet_Model_Parameter[selected_model][\"layers\"], channels=Triplet_Model_Parameter[selected_model][\"channels\"], max_pool=Triplet_Model_Parameter[selected_model][\"max_pool\"], embedding_size=Triplet_Model_Parameter[selected_model][\"embedding_size\"])\n",
    "\n",
    "try : \n",
    "    model = torch.load(\"./Tripletmaml_MINIIMAGENET_RetrievalTest_batchsize4_shots1.pt\")\n",
    "    model.to(device)\n",
    "except : \n",
    "    model = TripletCNN4(output_size= ways, hidden_size=Triplet_Model_Parameter[selected_model][\"hidden_size\"], layers=Triplet_Model_Parameter[selected_model][\"layers\"], channels=Triplet_Model_Parameter[selected_model][\"channels\"], max_pool=Triplet_Model_Parameter[selected_model][\"max_pool\"], embedding_size=Triplet_Model_Parameter[selected_model][\"embedding_size\"])\n",
    "    model.load_state_dict(torch.load(\"./Tripletmaml_MINIIMAGENET_RetrievalTest_batchsize4_shots1.pt\"))\n",
    "    model.to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(maml.parameters(), meta_lr) # meta-update\n",
    "triplet_w= Triplet_Model_Parameter[selected_model][\"lambda\"]\n",
    "combined_loss_fn= CombinedLoss2(triplet_w, shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Precision at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Precision at k\n",
    "    \"\"\"\n",
    "    intersection = np.intersect1d(y_true, y_pred[:k])\n",
    "    return len(intersection) / k\n",
    "\n",
    "def rel_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Relevance at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Relevance at k\n",
    "    \"\"\"\n",
    "    if y_pred[k-1] in y_true:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def average_precision_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Average Precision at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Average Precision at k\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    rel_counter = 0\n",
    "    for i in range(1, k+1):\n",
    "        ap += precision_at_k(y_true, y_pred, i) * rel_at_k(y_true, y_pred, i)\n",
    "        rel_counter += rel_at_k(y_true, y_pred, i)\n",
    "    #return ap / min(k, len(y_true))\n",
    "    if rel_counter == 0:\n",
    "        return  0\n",
    "    return ap / rel_counter\n",
    "\n",
    "\n",
    "def mean_average_precision(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes MAP at k\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            2D Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            2D Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           MAP at k\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean([average_precision_at_k(gt, pred, k) \\\n",
    "                    for gt, pred in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP_at_k(Fvec, imgLab,gt,rank=1,posonly=False):\n",
    "    # imgLab numpy.ndarray of shape: (8041,)\n",
    "   \n",
    "    N = len(imgLab) #8041 labels\n",
    "\n",
    "    imgLab = torch.LongTensor([imgLab[i] for i in range(len(imgLab))])\n",
    "    # imgLab.shape: [8041]\n",
    "    # Fvec.shape: [8041, 128]\n",
    "    \n",
    "    D = Fvec.mm(torch.t(Fvec)) # mm: matrix multiplication. (n×m) mm (m×p) results in  (n×p) tensor.\n",
    "    # [8041, 128] mm [128, 8041] --> [8041, 8041] this is D matrix\n",
    "    # There are 1's along the diagonal!\n",
    "    \n",
    "    D[torch.eye(len(imgLab)).bool()] = -1 \n",
    "    # torch.eye: Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "    # D[torch.eye(len(imgLab)).bool()]: diagonal elements of D will take a value of -1 ; the rest will remain the same\n",
    "    \n",
    "    #print(\"Distance Matris Shape\" , D.shape)\n",
    "    #print(D)\n",
    "    _,idx = D.topk(rank[-1])\n",
    "\n",
    "    if posonly == \"True\":\n",
    "        return mean_average_precision(np.array(gt)[:10],idx.numpy()[:10], k= rank[-1])\n",
    "    elif posonly == \"divide_by_class\" : \n",
    "        return [mean_average_precision(np.array(gt)[:10],idx.numpy()[:10], k= rank[-1]),mean_average_precision(np.array(gt)[10:20],idx.numpy()[10:20], k= rank[-1]),mean_average_precision(np.array(gt)[20:30],idx.numpy()[20:30], k= rank[-1]),mean_average_precision(np.array(gt)[30:40],idx.numpy()[30:40], k= rank[-1]),mean_average_precision(np.array(gt)[40:50],idx.numpy()[40:50], k= rank[-1]),mean_average_precision(np.array(gt),idx.numpy(), k= rank[-1])]\n",
    "    else :\n",
    "        return mean_average_precision(np.array(gt),idx.numpy(), k= rank[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "y_true = []\n",
    "for i in  range(50):\n",
    "    floor = math.floor(i/10) \n",
    "    max = ((floor + 1 ) * 10 ) \n",
    "\n",
    "    #print(\"for i.th element\" , str(i) , \" floor is \" , floor , \"max is \", max , \"floor * 10 = \" , floor*10 )\n",
    "    range_list = list(range(floor*10 , max))\n",
    "    range_list.remove(i)\n",
    "    y_true.append(range_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for testmap in [1,5,9]:\n",
    "    mean_map = []\n",
    "    with open(\"64_imagenet_all_1000iter_dividebyclass_MAP\"+str(testmap)+\".txt\",\"w\") as file :\n",
    "        for i in range(600):\n",
    "            print(i)\n",
    "            a = triplet_imagenet_dataset.sample(\"test\",mode=\"img_retrieval\",samples_per_class = 10)\n",
    "            opt.zero_grad() # for each batch, gradients should be cleaned.\n",
    "            # Compute meta-training loss\n",
    "            # print('Task no: ', task)\n",
    "            learner = maml.clone()\n",
    "            batch = a\n",
    "\n",
    "            train_error, _ = fast_adapt_image_retrieval(batch,\n",
    "                                                                learner,\n",
    "                                                                combined_loss_fn,\n",
    "                                                                adaptation_steps,\n",
    "                                                                shots,\n",
    "                                                                ways,\n",
    "                                                                device)                                                \n",
    "            embeddings = []\n",
    "            labels = torch.from_numpy(np.array(list(a[3])))\n",
    "            for idx,image in enumerate(a[2]):\n",
    "                embedding,class_prob =learner.forward_once(torch.unsqueeze(image,0).to(device)) \n",
    "                embedding = F.normalize(embedding, p = 2 , dim =1).cpu()\n",
    "                embeddings.append(np.array(embedding[0].tolist()))\n",
    "            embeddings = torch.from_numpy(np.array(embeddings))\n",
    "\n",
    "            map = mAP_at_k(embeddings,labels,y_true,[testmap],posonly=\"divide_by_class\")\n",
    "            file.write(\"iteration - \" + str(i) + \" mAP@\"+str(testmap)+\" :\" + str(map) + \"\\n\" )\n",
    "            mean_map.append(map)\n",
    "        print(np.mean(mean_map))\n",
    "        file.write(\"General Mean - \" + str(i) + \" mAP@\"+str(testmap)+\"  :\" + str(np.mean(mean_map)) + \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37bb63493fa6dddf35bfed9ad35536dbf43dc1c10eb467e9be32956dae9593c7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
