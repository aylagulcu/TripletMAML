{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas.core.common import flatten\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import learn2learn as l2l\n",
    "from losses import *\n",
    "from Triplets import *\n",
    "from cnn4_triplet import *\n",
    "\n",
    "import datasets\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triplet_Model_Parameter = {\n",
    "    \"CIFARFS\" : {\"data\" : TripletFSCIFAR100 , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":256,\"margin\":1.0,\"lambda\":1} ,\n",
    "    \"CUB\" : {\"data\" : TripletCUB , \"root\" : \"./data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "    \"FLOWERS\" : {\"data\" : TripletFlowers , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "    \"MINIIMAGENET\" : {\"data\" : TripletMiniImageNet , \"root\" : \"data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":32, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":800,\"margin\":1.0,\"lambda\":1},\n",
    "    \"OMNIGLOT\" : {\"data\" : TripletOmniglot , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor(),transforms.Resize((28,28))]), \"hidden_size\":64, \"layers\":4, \"channels\":1, \"max_pool\":False, \"embedding_size\":256,\"margin\":1.0,\"lambda\":1},\n",
    "    \"MINIIMAGENET_64\" : {\"data\" : TripletMiniImageNet , \"root\" : \"data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets, shots):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    # return (predictions == targets).sum().float() / targets.size(0)\n",
    "\n",
    "    if shots == 1:\n",
    "        mask = np.array([True, False, False, False, False, False, False, False, True, True, True, True])\n",
    "    else: # shots==5\n",
    "        mask = np.array(\n",
    "        [True, False, False, False, False, False, False, False, False, False, False, False,\tFalse, False, False, False, False, False, False, False, \n",
    "        True, False, False, False, False, True, False, False, False, False, True, False, False, False, False, True, False, False, False, False,\n",
    "        True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True ,True, True])\n",
    "    return (predictions[mask] == targets[mask]).sum().float() / targets[mask].size(0)\n",
    "\n",
    "\n",
    "def fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    adaptation_data, adaptation_labels, evaluation_data, evaluation_labels = batch\n",
    "    adaptation_data[0]= adaptation_data[0].to(device)\n",
    "    adaptation_data[1]= adaptation_data[1].to(device)\n",
    "    adaptation_data[2]= adaptation_data[2].to(device)\n",
    "\n",
    "    adaptation_labels= adaptation_labels.to(device)\n",
    "\n",
    "    evaluation_data[0]= evaluation_data[0].to(device)\n",
    "    evaluation_data[1]= evaluation_data[1].to(device)\n",
    "    evaluation_data[2]= evaluation_data[2].to(device)\n",
    "\n",
    "    evaluation_labels= evaluation_labels.to(device)\n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        out= learner(adaptation_data[0], adaptation_data[1], adaptation_data[2] )\n",
    "        train_error = loss(out[0],out[1],out[2], torch.vstack([out[3],out[4],out[5]]), torch.hstack([adaptation_labels[0],adaptation_labels[1],adaptation_labels[2]]))\n",
    "        learner.adapt(train_error, allow_unused=True)\n",
    "\n",
    "    # Evaluate the adapted model\n",
    "    predictions = learner(evaluation_data[0], evaluation_data[1], evaluation_data[2])\n",
    "    valid_error= loss(predictions[0],predictions[1],predictions[2], torch.vstack([predictions[3],predictions[4],predictions[5]]), torch.hstack([evaluation_labels[0],evaluation_labels[1],evaluation_labels[2]]))\n",
    "    valid_accuracy = accuracy(torch.vstack([predictions[3],predictions[4],predictions[5]]), torch.hstack([evaluation_labels[0],evaluation_labels[1],evaluation_labels[2]]), shots)\n",
    "    return valid_error, valid_accuracy\n",
    "\n",
    "def fast_adapt_image_retrieval(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    adaptation_data, adaptation_labels, evaluation_data, evaluation_labels = batch\n",
    "    adaptation_data[0]= adaptation_data[0].to(device)\n",
    "    adaptation_data[1]= adaptation_data[1].to(device)\n",
    "    adaptation_data[2]= adaptation_data[2].to(device)\n",
    "\n",
    "    adaptation_labels= adaptation_labels.to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        out= learner(adaptation_data[0], adaptation_data[1], adaptation_data[2] )\n",
    "        train_error = loss(out[0],out[1],out[2], torch.vstack([out[3],out[4],out[5]]), torch.hstack([adaptation_labels[0],adaptation_labels[1],adaptation_labels[2]]))\n",
    "        learner.adapt(train_error, allow_unused=True)\n",
    "\n",
    "   \n",
    "    return train_error, train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways=5\n",
    "shots=1\n",
    "meta_lr=0.001 # as in MAML\n",
    "fast_lr=0.01 # as in MAML\n",
    "meta_batch_size=4 # Maml Omniglot:32; miniImageNet: 4 \n",
    "adaptation_steps=15\n",
    "test_adaptation_steps=10\n",
    "num_iterations= 60000\n",
    "cuda=True\n",
    "seed=42\n",
    "num_test_episodes= 600\n",
    "selected_model = \"MINIIMAGENET_64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device('cuda')    \n",
    "\n",
    "triplet_imagenet_dataset = Triplet_Model_Parameter[selected_model][\"data\"](root = Triplet_Model_Parameter[selected_model][\"root\"], download = Triplet_Model_Parameter[selected_model][\"download\"], transform = Triplet_Model_Parameter[selected_model][\"transform\"])\n",
    "\n",
    "# Create model using saved parameters:\n",
    "model = TripletCNN4(output_size= ways, hidden_size=Triplet_Model_Parameter[selected_model][\"hidden_size\"], layers=Triplet_Model_Parameter[selected_model][\"layers\"], channels=Triplet_Model_Parameter[selected_model][\"channels\"], max_pool=Triplet_Model_Parameter[selected_model][\"max_pool\"], embedding_size=Triplet_Model_Parameter[selected_model][\"embedding_size\"])\n",
    "\n",
    "try : \n",
    "    model = torch.load(\"./Tripletmaml_MINIIMAGENET_RetrievalTest_batchsize4_shots1.pt\")\n",
    "    model.to(device)\n",
    "except : \n",
    "    model = TripletCNN4(output_size= ways, hidden_size=Triplet_Model_Parameter[selected_model][\"hidden_size\"], layers=Triplet_Model_Parameter[selected_model][\"layers\"], channels=Triplet_Model_Parameter[selected_model][\"channels\"], max_pool=Triplet_Model_Parameter[selected_model][\"max_pool\"], embedding_size=Triplet_Model_Parameter[selected_model][\"embedding_size\"])\n",
    "    model.load_state_dict(torch.load(\"./Tripletmaml_MINIIMAGENET_RetrievalTest_batchsize4_shots1.pt\"))\n",
    "    model.to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(maml.parameters(), meta_lr) # meta-update\n",
    "triplet_w= Triplet_Model_Parameter[selected_model][\"lambda\"]\n",
    "combined_loss_fn= CombinedLoss2(triplet_w, shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = triplet_imagenet_dataset.sample(\"test\",mode=\"img_retrieval\",samples_per_class = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_support = a[0]\n",
    "y_support = a[1]\n",
    "x_query   = a[2]\n",
    "y_query   = a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 84, 84])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_support[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 84, 84])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_support[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 84])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_support[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad() # for each batch, gradients should be cleaned.\n",
    "# Compute meta-training loss\n",
    "# print('Task no: ', task)\n",
    "learner = maml.clone()\n",
    "batch = a\n",
    "\n",
    "train_error, _ = fast_adapt_image_retrieval(batch,\n",
    "                                                    learner,\n",
    "                                                    combined_loss_fn,\n",
    "                                                    adaptation_steps,\n",
    "                                                    shots,\n",
    "                                                    ways,\n",
    "                                                    device)                                                \n",
    "embeddings = []\n",
    "labels = torch.from_numpy(np.array(list(a[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,image in enumerate(a[2]):\n",
    "    embedding,class_prob =learner.forward_once(torch.unsqueeze(image,0).to(device)) \n",
    "    embedding = F.normalize(embedding, p = 2 , dim =1).cpu()\n",
    "    embeddings.append(np.array(embedding[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.from_numpy(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1600])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(Fvec, imgLab,rank=None):\n",
    "    # imgLab numpy.ndarray of shape: (8041,)\n",
    "   \n",
    "    N = len(imgLab) #8041 labels\n",
    "\n",
    "    imgLab = torch.LongTensor([imgLab[i] for i in range(len(imgLab))])\n",
    "    # imgLab.shape: [8041]\n",
    "    # Fvec.shape: [8041, 128]\n",
    "    \n",
    "    D = Fvec.mm(torch.t(Fvec)) # mm: matrix multiplication. (n×m) mm (m×p) results in  (n×p) tensor.\n",
    "    # [8041, 128] mm [128, 8041] --> [8041, 8041] this is D matrix\n",
    "    # There are 1's along the diagonal!\n",
    "    \n",
    "    D[torch.eye(len(imgLab)).bool()] = -1 \n",
    "    # torch.eye: Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "    # D[torch.eye(len(imgLab)).bool()]: diagonal elements of D will take a value of -1 ; the rest will remain the same\n",
    "    \n",
    "    if rank==None: # only rank 1 is computed\n",
    "        _,idx = D.max(1) # returns both values and indices; dim=1 means returns for each row \n",
    "        imgPre = imgLab[idx]\n",
    "        A = (imgPre==imgLab).float()\n",
    "        return (torch.sum(A)/N).item()\n",
    "    else:\n",
    "        _,idx = D.topk(rank[-1])\n",
    "        acc_list = []\n",
    "        for r in rank:\n",
    "            A = 0\n",
    "            for i in range(r):\n",
    "                imgPre = imgLab[idx[:,i]]\n",
    "                A += (imgPre==imgLab).float()\n",
    "            acc_list.append((torch.sum((A>0).float())/N).item())\n",
    "        return torch.Tensor(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2800, 0.4200, 0.6000, 0.8600, 0.8800, 0.8800])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(embeddings, labels, rank= [1, 2, 4, 8,9, 10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n"
     ]
    }
   ],
   "source": [
    "recall_values = []\n",
    "\n",
    "for i in range(600):\n",
    "    print(i)\n",
    "    a = triplet_imagenet_dataset.sample(\"test\",mode=\"img_retrieval\",samples_per_class = 10)\n",
    "    opt.zero_grad() # for each batch, gradients should be cleaned.\n",
    "    # Compute meta-training loss\n",
    "    # print('Task no: ', task)\n",
    "    learner = maml.clone()\n",
    "    batch = a\n",
    "\n",
    "    train_error, _ = fast_adapt_image_retrieval(batch,\n",
    "                                                        learner,\n",
    "                                                        combined_loss_fn,\n",
    "                                                        adaptation_steps,\n",
    "                                                        shots,\n",
    "                                                        ways,\n",
    "                                                        device)                                                \n",
    "    embeddings = []\n",
    "    labels = torch.from_numpy(np.array(list(a[3])))\n",
    "    for idx,image in enumerate(a[2]):\n",
    "        embedding,class_prob =learner.forward_once(torch.unsqueeze(image,0).to(device)) \n",
    "        embedding = F.normalize(embedding, p = 2 , dim =1).cpu()\n",
    "        embeddings.append(np.array(embedding[0].tolist()))\n",
    "    embeddings = torch.from_numpy(np.array(embeddings))\n",
    "    recall_values.append(recall(embeddings, labels, rank= [1, 2, 4, 8, 9,  10]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.2800, 0.3600, 0.6200, 0.8800, 0.9200, 0.9200]),\n",
       " tensor([0.3000, 0.4000, 0.5600, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.4200, 0.5200, 0.6800, 0.8600, 0.9200, 0.9200]),\n",
       " tensor([0.2600, 0.3800, 0.7000, 0.8600, 0.8600, 0.8800]),\n",
       " tensor([0.2800, 0.3800, 0.5400, 0.7800, 0.7800, 0.8000]),\n",
       " tensor([0.2600, 0.3400, 0.5400, 0.8000, 0.8600, 0.9000]),\n",
       " tensor([0.2400, 0.4200, 0.5600, 0.8000, 0.8600, 0.9200]),\n",
       " tensor([0.1600, 0.3200, 0.5600, 0.8800, 0.9200, 0.9400]),\n",
       " tensor([0.1200, 0.3400, 0.5400, 0.8400, 0.9000, 0.9000]),\n",
       " tensor([0.2000, 0.3800, 0.6200, 0.8200, 0.8200, 0.8200]),\n",
       " tensor([0.1400, 0.3200, 0.5200, 0.7800, 0.8000, 0.8400]),\n",
       " tensor([0.2200, 0.3200, 0.5000, 0.8200, 0.8600, 0.9400]),\n",
       " tensor([0.1800, 0.3600, 0.5600, 0.8000, 0.8000, 0.8000]),\n",
       " tensor([0.3200, 0.4000, 0.5800, 0.7800, 0.8200, 0.8200]),\n",
       " tensor([0.2800, 0.4400, 0.6400, 0.8200, 0.8800, 0.8800]),\n",
       " tensor([0.2800, 0.3800, 0.5600, 0.8600, 0.8600, 0.8800]),\n",
       " tensor([0.2000, 0.3800, 0.5200, 0.7600, 0.7600, 0.7800]),\n",
       " tensor([0.2200, 0.4000, 0.5400, 0.7400, 0.7600, 0.8000]),\n",
       " tensor([0.2600, 0.4400, 0.6200, 0.8000, 0.8400, 0.8400]),\n",
       " tensor([0.3400, 0.5200, 0.6000, 0.8400, 0.8800, 0.9000]),\n",
       " tensor([0.2400, 0.4200, 0.6000, 0.7600, 0.8200, 0.8200]),\n",
       " tensor([0.2200, 0.3800, 0.6000, 0.8600, 0.9000, 0.9000]),\n",
       " tensor([0.2800, 0.4400, 0.6200, 0.8200, 0.8400, 0.8400]),\n",
       " tensor([0.1800, 0.3200, 0.5200, 0.7600, 0.8000, 0.8000]),\n",
       " tensor([0.3200, 0.5400, 0.6600, 0.8000, 0.8800, 0.9000]),\n",
       " tensor([0.3000, 0.4000, 0.6600, 0.8000, 0.8600, 0.8800]),\n",
       " tensor([0.2000, 0.4800, 0.6800, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.3600, 0.4800, 0.6200, 0.8000, 0.8000, 0.8400]),\n",
       " tensor([0.3400, 0.4400, 0.6400, 0.7400, 0.8000, 0.8200]),\n",
       " tensor([0.2600, 0.3800, 0.6400, 0.7600, 0.7800, 0.8000]),\n",
       " tensor([0.2600, 0.4000, 0.5600, 0.7600, 0.8000, 0.8600]),\n",
       " tensor([0.2400, 0.4800, 0.6600, 0.7800, 0.8200, 0.8400]),\n",
       " tensor([0.3000, 0.4400, 0.7000, 0.8400, 0.9000, 0.9000]),\n",
       " tensor([0.3400, 0.4600, 0.5400, 0.7200, 0.7800, 0.8000]),\n",
       " tensor([0.1800, 0.3200, 0.6000, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.2600, 0.4200, 0.5600, 0.7400, 0.7600, 0.8200]),\n",
       " tensor([0.2800, 0.4000, 0.7400, 0.8600, 0.8600, 0.8800]),\n",
       " tensor([0.2400, 0.4400, 0.6200, 0.7600, 0.8000, 0.8000]),\n",
       " tensor([0.3200, 0.3800, 0.5200, 0.7400, 0.7600, 0.8200]),\n",
       " tensor([0.2800, 0.3800, 0.6000, 0.7800, 0.8600, 0.9000]),\n",
       " tensor([0.2400, 0.3400, 0.5200, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.3400, 0.5000, 0.7000, 0.8000, 0.8400, 0.9000]),\n",
       " tensor([0.3400, 0.6000, 0.7000, 0.8600, 0.8800, 0.8800]),\n",
       " tensor([0.1800, 0.3400, 0.6000, 0.7600, 0.8000, 0.8200]),\n",
       " tensor([0.2800, 0.3800, 0.6200, 0.8000, 0.8200, 0.8200]),\n",
       " tensor([0.2400, 0.3800, 0.5200, 0.7200, 0.7200, 0.8200]),\n",
       " tensor([0.3400, 0.5200, 0.5800, 0.8000, 0.8400, 0.8400]),\n",
       " tensor([0.2800, 0.4400, 0.5400, 0.7200, 0.7600, 0.8000]),\n",
       " tensor([0.2600, 0.3600, 0.6000, 0.8200, 0.8600, 0.9200]),\n",
       " tensor([0.1600, 0.3600, 0.6400, 0.8400, 0.8400, 0.9000]),\n",
       " tensor([0.3400, 0.4600, 0.5600, 0.8200, 0.9000, 0.9000]),\n",
       " tensor([0.3400, 0.4200, 0.6000, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.2600, 0.4400, 0.5600, 0.7000, 0.7200, 0.7200]),\n",
       " tensor([0.2800, 0.4400, 0.5600, 0.7000, 0.7200, 0.7600]),\n",
       " tensor([0.2400, 0.4400, 0.5800, 0.7800, 0.8200, 0.8400]),\n",
       " tensor([0.2000, 0.3600, 0.6000, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.3000, 0.3400, 0.5600, 0.7600, 0.7800, 0.8600]),\n",
       " tensor([0.2000, 0.4200, 0.4800, 0.7200, 0.8000, 0.8400]),\n",
       " tensor([0.2200, 0.3800, 0.6200, 0.7800, 0.8000, 0.8000]),\n",
       " tensor([0.3000, 0.3600, 0.5200, 0.7400, 0.8200, 0.8400]),\n",
       " tensor([0.3400, 0.5000, 0.6200, 0.7400, 0.7800, 0.8000]),\n",
       " tensor([0.2400, 0.5000, 0.6400, 0.8800, 0.9200, 0.9400]),\n",
       " tensor([0.2400, 0.3600, 0.5600, 0.8200, 0.8400, 0.9000]),\n",
       " tensor([0.2400, 0.4200, 0.6000, 0.8200, 0.8200, 0.8400]),\n",
       " tensor([0.1600, 0.2600, 0.5000, 0.7600, 0.7800, 0.8200]),\n",
       " tensor([0.3000, 0.3600, 0.5400, 0.8000, 0.8600, 0.8800]),\n",
       " tensor([0.2200, 0.3400, 0.6000, 0.7400, 0.8200, 0.8200]),\n",
       " tensor([0.3200, 0.4600, 0.6600, 0.7800, 0.8000, 0.8600]),\n",
       " tensor([0.2800, 0.4400, 0.6200, 0.7200, 0.7200, 0.7800]),\n",
       " tensor([0.1800, 0.4600, 0.5600, 0.8400, 0.8400, 0.9000]),\n",
       " tensor([0.3400, 0.4800, 0.6600, 0.8600, 0.8600, 0.9200]),\n",
       " tensor([0.3000, 0.3400, 0.5400, 0.7400, 0.7800, 0.8000]),\n",
       " tensor([0.1800, 0.3600, 0.4600, 0.8000, 0.8200, 0.8200]),\n",
       " tensor([0.3000, 0.3600, 0.6200, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.2000, 0.4200, 0.5600, 0.8000, 0.8200, 0.8600]),\n",
       " tensor([0.2800, 0.4200, 0.5600, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.3400, 0.4200, 0.5200, 0.6800, 0.7000, 0.7400]),\n",
       " tensor([0.2000, 0.3600, 0.6600, 0.8200, 0.8400, 0.8800]),\n",
       " tensor([0.2000, 0.3400, 0.6400, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.3600, 0.4200, 0.5800, 0.8400, 0.9000, 0.9000]),\n",
       " tensor([0.2600, 0.4600, 0.6400, 0.8800, 0.9400, 0.9400]),\n",
       " tensor([0.5200, 0.5400, 0.7200, 0.8600, 0.8600, 0.9400]),\n",
       " tensor([0.2400, 0.3800, 0.5600, 0.7800, 0.8200, 0.8400]),\n",
       " tensor([0.3600, 0.5400, 0.6400, 0.8200, 0.8800, 0.8800]),\n",
       " tensor([0.3000, 0.3800, 0.5400, 0.7200, 0.7600, 0.8200]),\n",
       " tensor([0.2400, 0.3800, 0.5600, 0.7800, 0.7800, 0.7800]),\n",
       " tensor([0.3200, 0.4400, 0.6400, 0.8000, 0.8400, 0.8600]),\n",
       " tensor([0.2800, 0.4400, 0.6200, 0.8000, 0.8400, 0.9000]),\n",
       " tensor([0.3600, 0.5200, 0.6400, 0.8000, 0.8400, 0.8600]),\n",
       " tensor([0.3000, 0.5400, 0.6400, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.1600, 0.3600, 0.7000, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.2000, 0.3400, 0.6400, 0.8000, 0.8200, 0.8200]),\n",
       " tensor([0.2000, 0.3600, 0.6400, 0.8400, 0.8800, 0.9200]),\n",
       " tensor([0.2400, 0.4200, 0.5800, 0.8400, 0.9200, 0.9200]),\n",
       " tensor([0.1800, 0.3000, 0.6600, 0.8200, 0.9000, 0.9000]),\n",
       " tensor([0.3000, 0.5200, 0.6600, 0.7800, 0.8200, 0.8600]),\n",
       " tensor([0.2000, 0.3400, 0.5400, 0.7800, 0.8800, 0.9200]),\n",
       " tensor([0.3600, 0.5400, 0.7400, 0.8600, 0.9000, 0.9200]),\n",
       " tensor([0.2000, 0.4600, 0.5600, 0.8800, 0.8800, 0.8800]),\n",
       " tensor([0.2600, 0.5000, 0.6400, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.2800, 0.4000, 0.6600, 0.9000, 0.9200, 0.9400]),\n",
       " tensor([0.2400, 0.3400, 0.5800, 0.7600, 0.7800, 0.8400]),\n",
       " tensor([0.2800, 0.3600, 0.5600, 0.8000, 0.8200, 0.8800]),\n",
       " tensor([0.2000, 0.4400, 0.5600, 0.7800, 0.8000, 0.8200]),\n",
       " tensor([0.1800, 0.3200, 0.5200, 0.8200, 0.8200, 0.8200]),\n",
       " tensor([0.1800, 0.3600, 0.5400, 0.8000, 0.8000, 0.8600]),\n",
       " tensor([0.3000, 0.4600, 0.5800, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.3200, 0.5200, 0.6600, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.3600, 0.5200, 0.6800, 0.8000, 0.8200, 0.8600]),\n",
       " tensor([0.2200, 0.3400, 0.5400, 0.7400, 0.8400, 0.8800]),\n",
       " tensor([0.2800, 0.4000, 0.5200, 0.8200, 0.8600, 0.9200]),\n",
       " tensor([0.2600, 0.3200, 0.3800, 0.6800, 0.7000, 0.7200]),\n",
       " tensor([0.2200, 0.4200, 0.6200, 0.7800, 0.8000, 0.8600]),\n",
       " tensor([0.3400, 0.4400, 0.6600, 0.8400, 0.8800, 0.9000]),\n",
       " tensor([0.2800, 0.4200, 0.6600, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.3200, 0.4800, 0.7200, 0.8600, 0.9000, 0.9400]),\n",
       " tensor([0.3400, 0.4200, 0.5200, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.3200, 0.5000, 0.6400, 0.7800, 0.8200, 0.8600]),\n",
       " tensor([0.1600, 0.2600, 0.5400, 0.9400, 0.9600, 0.9600]),\n",
       " tensor([0.2600, 0.5000, 0.6400, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.3600, 0.5200, 0.6800, 0.8400, 0.8600, 0.9000]),\n",
       " tensor([0.3000, 0.4800, 0.5600, 0.7800, 0.8000, 0.8200]),\n",
       " tensor([0.2400, 0.3600, 0.5600, 0.7800, 0.8600, 0.8600]),\n",
       " tensor([0.2800, 0.4200, 0.6000, 0.8800, 0.8800, 0.8800]),\n",
       " tensor([0.3000, 0.3800, 0.5600, 0.6800, 0.7200, 0.7800]),\n",
       " tensor([0.2600, 0.4400, 0.5400, 0.8200, 0.8800, 0.9200]),\n",
       " tensor([0.3600, 0.5600, 0.7000, 0.8600, 0.8600, 0.8800]),\n",
       " tensor([0.2800, 0.4600, 0.5600, 0.7200, 0.7600, 0.8000]),\n",
       " tensor([0.2600, 0.4200, 0.5600, 0.8200, 0.8400, 0.9000]),\n",
       " tensor([0.3800, 0.5600, 0.7000, 0.7800, 0.7800, 0.8200]),\n",
       " tensor([0.1400, 0.3200, 0.5800, 0.7600, 0.7800, 0.7800]),\n",
       " tensor([0.3200, 0.5000, 0.6800, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.2800, 0.4400, 0.5800, 0.7000, 0.7800, 0.7800]),\n",
       " tensor([0.2000, 0.3000, 0.4800, 0.7600, 0.8600, 0.9200]),\n",
       " tensor([0.2800, 0.3800, 0.5800, 0.7600, 0.7800, 0.8400]),\n",
       " tensor([0.3200, 0.4600, 0.6200, 0.8600, 0.8800, 0.9200]),\n",
       " tensor([0.2800, 0.4400, 0.6200, 0.8200, 0.8400, 0.8800]),\n",
       " tensor([0.2800, 0.5400, 0.6600, 0.8200, 0.8800, 0.8800]),\n",
       " tensor([0.2200, 0.3400, 0.5400, 0.8400, 0.9200, 0.9400]),\n",
       " tensor([0.3200, 0.5200, 0.6400, 0.8000, 0.9000, 0.9000]),\n",
       " tensor([0.2600, 0.3800, 0.6200, 0.7800, 0.7800, 0.8400]),\n",
       " tensor([0.2600, 0.4200, 0.6000, 0.8000, 0.8000, 0.8400]),\n",
       " tensor([0.1800, 0.3200, 0.5200, 0.8000, 0.8600, 0.9000]),\n",
       " tensor([0.1200, 0.2600, 0.4400, 0.7600, 0.7800, 0.7800]),\n",
       " tensor([0.3200, 0.4800, 0.6600, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.2600, 0.4400, 0.6000, 0.8600, 0.9000, 0.9200]),\n",
       " tensor([0.3000, 0.3800, 0.5200, 0.7800, 0.8000, 0.8400]),\n",
       " tensor([0.2000, 0.3600, 0.5200, 0.7200, 0.7600, 0.7600]),\n",
       " tensor([0.3000, 0.4600, 0.6400, 0.8200, 0.8800, 0.9000]),\n",
       " tensor([0.1600, 0.2600, 0.6000, 0.9200, 0.9200, 0.9400]),\n",
       " tensor([0.2400, 0.4000, 0.5000, 0.7400, 0.7600, 0.7600]),\n",
       " tensor([0.2000, 0.3600, 0.5400, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.2200, 0.4000, 0.5000, 0.7400, 0.8000, 0.8400]),\n",
       " tensor([0.3000, 0.4200, 0.7000, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.3600, 0.5000, 0.7200, 0.9000, 0.9000, 0.9000]),\n",
       " tensor([0.3400, 0.5600, 0.6800, 0.8200, 0.8800, 0.9000]),\n",
       " tensor([0.2800, 0.3400, 0.6200, 0.8600, 0.8600, 0.9000]),\n",
       " tensor([0.3200, 0.3800, 0.6400, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.1600, 0.3200, 0.5800, 0.8200, 0.8600, 0.8600]),\n",
       " tensor([0.3000, 0.3800, 0.5600, 0.7800, 0.8000, 0.8400]),\n",
       " tensor([0.3400, 0.4800, 0.6400, 0.8200, 0.8200, 0.8800]),\n",
       " tensor([0.3600, 0.5000, 0.6400, 0.9400, 0.9400, 0.9600]),\n",
       " tensor([0.2800, 0.3800, 0.6600, 0.8000, 0.8200, 0.8200]),\n",
       " tensor([0.4400, 0.5200, 0.6800, 0.7800, 0.8000, 0.8600]),\n",
       " tensor([0.3400, 0.4400, 0.6000, 0.7400, 0.8400, 0.8800]),\n",
       " tensor([0.2200, 0.3200, 0.5000, 0.7400, 0.8000, 0.8600]),\n",
       " tensor([0.3200, 0.4800, 0.7200, 0.8800, 0.8800, 0.8800]),\n",
       " tensor([0.3400, 0.4000, 0.5400, 0.8200, 0.8400, 0.8800]),\n",
       " tensor([0.1800, 0.2600, 0.5200, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.3200, 0.4200, 0.5400, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.3600, 0.5400, 0.6800, 0.8200, 0.8200, 0.8400]),\n",
       " tensor([0.3400, 0.4600, 0.5600, 0.8200, 0.8600, 0.9000]),\n",
       " tensor([0.3800, 0.4600, 0.6200, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.2800, 0.5000, 0.7200, 0.8400, 0.8800, 0.9000]),\n",
       " tensor([0.2200, 0.3600, 0.5800, 0.7400, 0.7800, 0.7800]),\n",
       " tensor([0.3600, 0.4600, 0.6200, 0.7600, 0.8000, 0.8000]),\n",
       " tensor([0.1400, 0.2800, 0.5800, 0.8000, 0.8800, 0.9000]),\n",
       " tensor([0.3000, 0.4000, 0.6000, 0.7600, 0.8400, 0.8400]),\n",
       " tensor([0.1200, 0.4000, 0.5400, 0.8600, 0.9200, 0.9200]),\n",
       " tensor([0.3400, 0.4800, 0.6600, 0.8200, 0.8800, 0.8800]),\n",
       " tensor([0.2200, 0.3600, 0.5400, 0.7800, 0.8200, 0.8600]),\n",
       " tensor([0.2800, 0.4400, 0.6600, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.2200, 0.3600, 0.6400, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.4400, 0.5400, 0.7000, 0.8600, 0.8600, 0.8600]),\n",
       " tensor([0.2600, 0.4800, 0.6000, 0.8000, 0.8400, 0.8400]),\n",
       " tensor([0.2400, 0.4600, 0.5600, 0.7600, 0.8000, 0.8000]),\n",
       " tensor([0.2200, 0.3600, 0.6200, 0.7400, 0.8400, 0.8600]),\n",
       " tensor([0.3200, 0.4800, 0.6200, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.1800, 0.4200, 0.6000, 0.7600, 0.7600, 0.8400]),\n",
       " tensor([0.2000, 0.3000, 0.5800, 0.8400, 0.9000, 0.9000]),\n",
       " tensor([0.2400, 0.3600, 0.6600, 0.8600, 0.9000, 0.9400]),\n",
       " tensor([0.1800, 0.3400, 0.5600, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.1200, 0.3000, 0.5000, 0.6800, 0.7600, 0.7800]),\n",
       " tensor([0.3600, 0.4600, 0.5800, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.2600, 0.4000, 0.6000, 0.7800, 0.8400, 0.8400]),\n",
       " tensor([0.1600, 0.2600, 0.4400, 0.8000, 0.8600, 0.8800]),\n",
       " tensor([0.2600, 0.4400, 0.7200, 0.8200, 0.9000, 0.9400]),\n",
       " tensor([0.3200, 0.4800, 0.7400, 0.9200, 0.9200, 0.9400]),\n",
       " tensor([0.3000, 0.4400, 0.6200, 0.8200, 0.8200, 0.8200]),\n",
       " tensor([0.2800, 0.4000, 0.5800, 0.7200, 0.7200, 0.7800]),\n",
       " tensor([0.3400, 0.4800, 0.7000, 0.8800, 0.9400, 0.9400]),\n",
       " tensor([0.3200, 0.4800, 0.6200, 0.7600, 0.8200, 0.8200]),\n",
       " tensor([0.2400, 0.4400, 0.5800, 0.7800, 0.8400, 0.8600]),\n",
       " tensor([0.4000, 0.5000, 0.6800, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.2800, 0.3800, 0.5600, 0.7400, 0.7600, 0.7800]),\n",
       " tensor([0.3400, 0.4800, 0.6800, 0.8400, 0.8800, 0.9200]),\n",
       " tensor([0.2800, 0.3600, 0.5800, 0.8000, 0.8600, 0.9200]),\n",
       " tensor([0.3000, 0.3600, 0.5000, 0.7000, 0.7200, 0.7400]),\n",
       " tensor([0.2600, 0.4800, 0.7000, 0.8600, 0.8800, 0.8800]),\n",
       " tensor([0.2600, 0.4200, 0.6800, 0.8000, 0.8400, 0.8400]),\n",
       " tensor([0.2800, 0.5200, 0.6800, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.2600, 0.3800, 0.5800, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.3200, 0.4400, 0.6800, 0.9000, 0.9200, 0.9400]),\n",
       " tensor([0.2600, 0.4400, 0.5800, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.3000, 0.4400, 0.6000, 0.7600, 0.8200, 0.8600]),\n",
       " tensor([0.2200, 0.3000, 0.5600, 0.8800, 0.9200, 0.9800]),\n",
       " tensor([0.3000, 0.4000, 0.6400, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.3600, 0.5000, 0.6600, 0.8400, 0.8400, 0.8600]),\n",
       " tensor([0.3200, 0.3600, 0.5200, 0.7400, 0.7800, 0.8000]),\n",
       " tensor([0.2400, 0.3400, 0.5800, 0.7400, 0.7800, 0.8000]),\n",
       " tensor([0.2000, 0.3600, 0.6000, 0.8400, 0.8600, 0.9000]),\n",
       " tensor([0.4200, 0.5200, 0.6400, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.3000, 0.4400, 0.6200, 0.8000, 0.8000, 0.8200]),\n",
       " tensor([0.2600, 0.3600, 0.5400, 0.7400, 0.7800, 0.8200]),\n",
       " tensor([0.3800, 0.4800, 0.6400, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.2400, 0.4200, 0.6200, 0.7800, 0.8000, 0.8600]),\n",
       " tensor([0.1800, 0.3800, 0.5800, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.3000, 0.4000, 0.6600, 0.7800, 0.8400, 0.8600]),\n",
       " tensor([0.2800, 0.4800, 0.7400, 0.9000, 0.9400, 0.9600]),\n",
       " tensor([0.2800, 0.4600, 0.6400, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.3400, 0.4200, 0.5800, 0.7600, 0.8200, 0.8800]),\n",
       " tensor([0.3800, 0.4600, 0.5800, 0.7800, 0.8200, 0.8800]),\n",
       " tensor([0.3400, 0.4200, 0.6200, 0.7800, 0.8800, 0.9200]),\n",
       " tensor([0.2800, 0.4000, 0.6600, 0.8600, 0.9000, 0.9000]),\n",
       " tensor([0.3000, 0.4400, 0.6000, 0.8000, 0.8400, 0.8600]),\n",
       " tensor([0.4200, 0.5400, 0.6800, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.2800, 0.3800, 0.6400, 0.7000, 0.7600, 0.8000]),\n",
       " tensor([0.3000, 0.4800, 0.7000, 0.8400, 0.8400, 0.8600]),\n",
       " tensor([0.3200, 0.5400, 0.7000, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.1400, 0.4200, 0.5400, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.2400, 0.4000, 0.6000, 0.8800, 0.9600, 0.9600]),\n",
       " tensor([0.3000, 0.4400, 0.6400, 0.8600, 0.8600, 0.8600]),\n",
       " tensor([0.3200, 0.5000, 0.6600, 0.8400, 0.8400, 0.8800]),\n",
       " tensor([0.0800, 0.2000, 0.5800, 0.8600, 0.9200, 0.9400]),\n",
       " tensor([0.2800, 0.4600, 0.6600, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.3000, 0.5000, 0.7000, 0.8200, 0.8400, 0.8800]),\n",
       " tensor([0.2600, 0.4800, 0.6200, 0.8200, 0.8800, 0.8800]),\n",
       " tensor([0.2800, 0.4000, 0.5800, 0.8600, 0.8600, 0.8800]),\n",
       " tensor([0.3000, 0.3600, 0.5000, 0.7600, 0.7600, 0.8200]),\n",
       " tensor([0.3400, 0.4800, 0.7200, 0.8600, 0.8800, 0.8800]),\n",
       " tensor([0.2000, 0.3000, 0.5600, 0.7800, 0.9000, 0.9200]),\n",
       " tensor([0.2800, 0.4000, 0.5800, 0.7600, 0.7600, 0.7600]),\n",
       " tensor([0.2400, 0.4600, 0.7000, 0.8600, 0.9000, 0.9400]),\n",
       " tensor([0.4600, 0.5800, 0.6400, 0.8000, 0.8200, 0.8200]),\n",
       " tensor([0.1600, 0.3400, 0.4800, 0.7600, 0.8000, 0.8400]),\n",
       " tensor([0.2400, 0.4200, 0.5800, 0.8400, 0.8400, 0.8400]),\n",
       " tensor([0.1800, 0.3200, 0.5600, 0.7200, 0.7400, 0.7600]),\n",
       " tensor([0.2400, 0.3800, 0.6200, 0.8000, 0.8400, 0.8400]),\n",
       " tensor([0.2400, 0.3800, 0.6000, 0.7800, 0.8200, 0.8200]),\n",
       " tensor([0.1200, 0.2000, 0.6000, 0.8600, 0.9000, 0.9200]),\n",
       " tensor([0.2000, 0.3600, 0.6400, 0.8000, 0.8800, 0.8800]),\n",
       " tensor([0.1800, 0.3200, 0.5800, 0.8600, 0.8800, 0.9200]),\n",
       " tensor([0.2400, 0.3800, 0.6200, 0.7600, 0.8000, 0.8400]),\n",
       " tensor([0.3400, 0.4200, 0.6200, 0.9000, 0.9400, 0.9600]),\n",
       " tensor([0.2800, 0.4400, 0.6400, 0.8400, 0.8400, 0.8600]),\n",
       " tensor([0.3000, 0.4400, 0.6800, 0.8000, 0.8200, 0.8200]),\n",
       " tensor([0.2800, 0.4000, 0.6000, 0.7600, 0.8000, 0.8600]),\n",
       " tensor([0.3800, 0.4600, 0.6600, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.2200, 0.4000, 0.6600, 0.8200, 0.8800, 0.8800]),\n",
       " tensor([0.3000, 0.5000, 0.6400, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.2400, 0.4600, 0.6200, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.2600, 0.4400, 0.6400, 0.8000, 0.8400, 0.8600]),\n",
       " tensor([0.2000, 0.3600, 0.6200, 0.7600, 0.8000, 0.8400]),\n",
       " tensor([0.2800, 0.4000, 0.5200, 0.7000, 0.7400, 0.7800]),\n",
       " tensor([0.3000, 0.4000, 0.5200, 0.8000, 0.8400, 0.8600]),\n",
       " tensor([0.2800, 0.5200, 0.7400, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.2600, 0.4600, 0.6000, 0.8200, 0.8600, 0.8600]),\n",
       " tensor([0.2400, 0.3600, 0.6400, 0.8400, 0.8800, 0.9000]),\n",
       " tensor([0.2600, 0.4400, 0.6200, 0.7600, 0.8400, 0.8800]),\n",
       " tensor([0.3600, 0.5800, 0.6600, 0.8000, 0.8400, 0.9000]),\n",
       " tensor([0.3000, 0.4400, 0.5400, 0.7400, 0.8400, 0.8400]),\n",
       " tensor([0.2800, 0.3800, 0.5200, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.3000, 0.4400, 0.5800, 0.8000, 0.8000, 0.8000]),\n",
       " tensor([0.3400, 0.4800, 0.5400, 0.7000, 0.7400, 0.8000]),\n",
       " tensor([0.2000, 0.3000, 0.5400, 0.8200, 0.8200, 0.8200]),\n",
       " tensor([0.4200, 0.4800, 0.6400, 0.7600, 0.8000, 0.8200]),\n",
       " tensor([0.2200, 0.4800, 0.7200, 0.8400, 0.8400, 0.8400]),\n",
       " tensor([0.3200, 0.4200, 0.5600, 0.8400, 0.9000, 0.9200]),\n",
       " tensor([0.1800, 0.3600, 0.6200, 0.9000, 0.9000, 0.9000]),\n",
       " tensor([0.2800, 0.4800, 0.7000, 0.8800, 0.8800, 0.8800]),\n",
       " tensor([0.3400, 0.4600, 0.6200, 0.8600, 0.8800, 0.8800]),\n",
       " tensor([0.3600, 0.4600, 0.5600, 0.7600, 0.8400, 0.9000]),\n",
       " tensor([0.2000, 0.4600, 0.6400, 0.8600, 0.9200, 0.9400]),\n",
       " tensor([0.2200, 0.4000, 0.5800, 0.7800, 0.8200, 0.8600]),\n",
       " tensor([0.2200, 0.5000, 0.7000, 0.8200, 0.8600, 0.8600]),\n",
       " tensor([0.3000, 0.5000, 0.7400, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.3200, 0.4600, 0.6600, 0.8600, 0.9000, 0.9000]),\n",
       " tensor([0.2400, 0.3800, 0.5000, 0.7600, 0.7800, 0.8200]),\n",
       " tensor([0.2800, 0.3800, 0.5000, 0.7000, 0.7600, 0.8200]),\n",
       " tensor([0.2000, 0.3200, 0.6200, 0.8200, 0.8800, 0.9200]),\n",
       " tensor([0.2600, 0.4000, 0.6600, 0.8600, 0.9000, 0.9200]),\n",
       " tensor([0.2400, 0.3800, 0.6400, 0.7800, 0.8200, 0.8400]),\n",
       " tensor([0.2000, 0.3800, 0.6000, 0.7600, 0.8000, 0.8200]),\n",
       " tensor([0.2800, 0.3600, 0.5400, 0.8400, 0.8400, 0.8800]),\n",
       " tensor([0.3800, 0.5600, 0.7000, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.3000, 0.4200, 0.6800, 0.8200, 0.8400, 0.8400]),\n",
       " tensor([0.2400, 0.3400, 0.5000, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.2800, 0.4600, 0.7000, 0.9000, 0.9000, 0.9200]),\n",
       " tensor([0.3200, 0.3800, 0.5800, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.2000, 0.3200, 0.6200, 0.7800, 0.8200, 0.8800]),\n",
       " tensor([0.2800, 0.3800, 0.5600, 0.7200, 0.8000, 0.8400]),\n",
       " tensor([0.2200, 0.4400, 0.6000, 0.7600, 0.8000, 0.8800]),\n",
       " tensor([0.3800, 0.5400, 0.6800, 0.8400, 0.8400, 0.8400]),\n",
       " tensor([0.3400, 0.4800, 0.6600, 0.8400, 0.8400, 0.8600]),\n",
       " tensor([0.2600, 0.4000, 0.5800, 0.7600, 0.8200, 0.9200]),\n",
       " tensor([0.3200, 0.4600, 0.6800, 0.8600, 0.8600, 0.9000]),\n",
       " tensor([0.3200, 0.4800, 0.7000, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.2800, 0.4200, 0.7200, 0.9400, 0.9400, 0.9400]),\n",
       " tensor([0.3600, 0.4600, 0.6600, 0.8200, 0.8200, 0.8400]),\n",
       " tensor([0.3800, 0.4600, 0.5600, 0.7000, 0.7600, 0.8400]),\n",
       " tensor([0.2800, 0.3800, 0.6000, 0.8200, 0.8400, 0.8800]),\n",
       " tensor([0.2400, 0.3200, 0.5400, 0.7400, 0.7400, 0.7800]),\n",
       " tensor([0.3200, 0.3800, 0.5400, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.3000, 0.5200, 0.6800, 0.9400, 0.9400, 0.9400]),\n",
       " tensor([0.2800, 0.4600, 0.6200, 0.7400, 0.7600, 0.8200]),\n",
       " tensor([0.2600, 0.4600, 0.7000, 0.8400, 0.9000, 0.9400]),\n",
       " tensor([0.2800, 0.3800, 0.5600, 0.7200, 0.8200, 0.8400]),\n",
       " tensor([0.3000, 0.4200, 0.5200, 0.7600, 0.8200, 0.8400]),\n",
       " tensor([0.2200, 0.4200, 0.6400, 0.8200, 0.8600, 0.9000]),\n",
       " tensor([0.3200, 0.4200, 0.5200, 0.7400, 0.8000, 0.8200]),\n",
       " tensor([0.2600, 0.4600, 0.5200, 0.7800, 0.8400, 0.8400]),\n",
       " tensor([0.2600, 0.3800, 0.5400, 0.7800, 0.7800, 0.8000]),\n",
       " tensor([0.2600, 0.4600, 0.5800, 0.7400, 0.7400, 0.7600]),\n",
       " tensor([0.2400, 0.4000, 0.6600, 0.8200, 0.8400, 0.8800]),\n",
       " tensor([0.3800, 0.4800, 0.6400, 0.7400, 0.7800, 0.8200]),\n",
       " tensor([0.3000, 0.4600, 0.6000, 0.8200, 0.8600, 0.8600]),\n",
       " tensor([0.3200, 0.4800, 0.6000, 0.7600, 0.7800, 0.7800]),\n",
       " tensor([0.1600, 0.3400, 0.5200, 0.6600, 0.6600, 0.7200]),\n",
       " tensor([0.2400, 0.3800, 0.5400, 0.7600, 0.7800, 0.8200]),\n",
       " tensor([0.2600, 0.4000, 0.6000, 0.7800, 0.8200, 0.8200]),\n",
       " tensor([0.2800, 0.4400, 0.6200, 0.8400, 0.8800, 0.9200]),\n",
       " tensor([0.2600, 0.3800, 0.6400, 0.8200, 0.8800, 0.9000]),\n",
       " tensor([0.2000, 0.3400, 0.6400, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.2200, 0.3600, 0.5600, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.1200, 0.3400, 0.5600, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.1400, 0.3800, 0.5800, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.3400, 0.4800, 0.7200, 0.7800, 0.8000, 0.8000]),\n",
       " tensor([0.3600, 0.5000, 0.5400, 0.7800, 0.8200, 0.8800]),\n",
       " tensor([0.3000, 0.4000, 0.6200, 0.8800, 0.9400, 0.9600]),\n",
       " tensor([0.3000, 0.4400, 0.5800, 0.7400, 0.8000, 0.8000]),\n",
       " tensor([0.3000, 0.5000, 0.7200, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.2600, 0.4000, 0.5800, 0.7000, 0.7600, 0.7600]),\n",
       " tensor([0.2600, 0.4200, 0.6000, 0.8400, 0.8400, 0.8600]),\n",
       " tensor([0.3800, 0.5800, 0.7800, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.2000, 0.3800, 0.6000, 0.8800, 0.9200, 0.9400]),\n",
       " tensor([0.2600, 0.4200, 0.7000, 0.8200, 0.8800, 0.8800]),\n",
       " tensor([0.2400, 0.3200, 0.5200, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.1600, 0.4200, 0.5000, 0.7600, 0.8000, 0.8400]),\n",
       " tensor([0.3600, 0.4400, 0.6000, 0.7800, 0.8400, 0.8600]),\n",
       " tensor([0.3400, 0.5000, 0.6800, 0.8600, 0.8600, 0.8600]),\n",
       " tensor([0.1600, 0.3600, 0.6400, 0.8200, 0.8800, 0.9200]),\n",
       " tensor([0.1400, 0.3000, 0.6400, 0.7800, 0.8600, 0.9000]),\n",
       " tensor([0.3400, 0.5000, 0.6000, 0.8000, 0.8600, 0.9000]),\n",
       " tensor([0.3200, 0.4800, 0.6400, 0.8200, 0.8400, 0.9000]),\n",
       " tensor([0.3600, 0.4800, 0.7600, 0.9000, 0.9600, 0.9600]),\n",
       " tensor([0.1800, 0.3600, 0.5600, 0.7800, 0.7800, 0.8000]),\n",
       " tensor([0.2600, 0.3600, 0.5200, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.2200, 0.3800, 0.6600, 0.8200, 0.8600, 0.8600]),\n",
       " tensor([0.2200, 0.4000, 0.7000, 0.7400, 0.8000, 0.8200]),\n",
       " tensor([0.2200, 0.3200, 0.5200, 0.8000, 0.8000, 0.8200]),\n",
       " tensor([0.3200, 0.4200, 0.6000, 0.7600, 0.7800, 0.8000]),\n",
       " tensor([0.2000, 0.3200, 0.6000, 0.9000, 0.9000, 0.9000]),\n",
       " tensor([0.3400, 0.4400, 0.6000, 0.7400, 0.8200, 0.8600]),\n",
       " tensor([0.2000, 0.3800, 0.5400, 0.7800, 0.8600, 0.8600]),\n",
       " tensor([0.3600, 0.5000, 0.6200, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.2000, 0.4400, 0.7000, 0.8800, 0.9200, 0.9200]),\n",
       " tensor([0.3200, 0.4000, 0.7200, 0.8600, 0.8600, 0.9000]),\n",
       " tensor([0.3600, 0.5000, 0.6400, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.2800, 0.4800, 0.7200, 0.9400, 0.9400, 0.9400]),\n",
       " tensor([0.2000, 0.3600, 0.6000, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.2800, 0.3600, 0.5400, 0.8200, 0.8200, 0.8400]),\n",
       " tensor([0.2400, 0.3600, 0.5400, 0.8400, 0.9200, 0.9600]),\n",
       " tensor([0.2400, 0.3600, 0.6200, 0.8000, 0.8400, 0.8400]),\n",
       " tensor([0.1400, 0.3600, 0.6200, 0.7600, 0.8000, 0.8000]),\n",
       " tensor([0.2800, 0.4400, 0.5600, 0.7600, 0.8200, 0.8200]),\n",
       " tensor([0.3000, 0.4800, 0.6800, 0.7400, 0.7600, 0.7800]),\n",
       " tensor([0.4000, 0.5400, 0.6400, 0.7800, 0.8200, 0.8400]),\n",
       " tensor([0.3800, 0.5600, 0.7600, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.3000, 0.4400, 0.5600, 0.8800, 0.9000, 0.9000]),\n",
       " tensor([0.3000, 0.3800, 0.6200, 0.7800, 0.8200, 0.8800]),\n",
       " tensor([0.2400, 0.4200, 0.7000, 0.8800, 0.8800, 0.8800]),\n",
       " tensor([0.1800, 0.3000, 0.5200, 0.7200, 0.8000, 0.8600]),\n",
       " tensor([0.2800, 0.4600, 0.6200, 0.7600, 0.7800, 0.8200]),\n",
       " tensor([0.2800, 0.4200, 0.6000, 0.7600, 0.7800, 0.8000]),\n",
       " tensor([0.2800, 0.4600, 0.5200, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.3400, 0.3800, 0.5400, 0.7800, 0.8000, 0.8200]),\n",
       " tensor([0.5400, 0.6400, 0.7600, 0.8000, 0.8000, 0.8400]),\n",
       " tensor([0.2800, 0.4600, 0.6600, 0.9000, 0.9000, 0.9000]),\n",
       " tensor([0.2800, 0.4400, 0.6600, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.2400, 0.3600, 0.5000, 0.7400, 0.8000, 0.8000]),\n",
       " tensor([0.3400, 0.5200, 0.8000, 0.8400, 0.9000, 0.9600]),\n",
       " tensor([0.2200, 0.4200, 0.6000, 0.8400, 0.8600, 0.9000]),\n",
       " tensor([0.2800, 0.4000, 0.5800, 0.7400, 0.8000, 0.8400]),\n",
       " tensor([0.3000, 0.4600, 0.6400, 0.7200, 0.8000, 0.8200]),\n",
       " tensor([0.2000, 0.3400, 0.6000, 0.7800, 0.8600, 0.9200]),\n",
       " tensor([0.3000, 0.4200, 0.5800, 0.8200, 0.8200, 0.8400]),\n",
       " tensor([0.2800, 0.4800, 0.6200, 0.8200, 0.8400, 0.8400]),\n",
       " tensor([0.2800, 0.4600, 0.5600, 0.7200, 0.8000, 0.8200]),\n",
       " tensor([0.2200, 0.3800, 0.6000, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.2800, 0.4400, 0.6400, 0.8400, 0.8400, 0.8600]),\n",
       " tensor([0.3200, 0.5200, 0.6600, 0.8600, 0.9400, 0.9400]),\n",
       " tensor([0.3200, 0.4800, 0.6600, 0.7800, 0.8000, 0.8000]),\n",
       " tensor([0.1600, 0.3000, 0.5600, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.2600, 0.4000, 0.5400, 0.7000, 0.7800, 0.7800]),\n",
       " tensor([0.3000, 0.5000, 0.6600, 0.7800, 0.8200, 0.8800]),\n",
       " tensor([0.4000, 0.5400, 0.6400, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.3200, 0.3600, 0.5800, 0.7200, 0.7400, 0.8600]),\n",
       " tensor([0.3800, 0.4800, 0.5200, 0.6800, 0.7000, 0.7200]),\n",
       " tensor([0.3600, 0.5000, 0.6400, 0.8400, 0.8400, 0.8800]),\n",
       " tensor([0.2600, 0.4400, 0.6000, 0.8000, 0.8200, 0.8600]),\n",
       " tensor([0.2000, 0.2600, 0.5200, 0.7800, 0.8000, 0.8000]),\n",
       " tensor([0.3000, 0.4400, 0.7000, 0.8600, 0.8800, 0.8800]),\n",
       " tensor([0.2400, 0.4000, 0.6600, 0.8200, 0.8600, 0.9000]),\n",
       " tensor([0.2000, 0.3800, 0.5800, 0.7600, 0.8200, 0.8400]),\n",
       " tensor([0.2400, 0.3400, 0.5200, 0.7600, 0.8200, 0.8600]),\n",
       " tensor([0.3800, 0.4200, 0.5800, 0.8400, 0.8600, 0.9000]),\n",
       " tensor([0.2800, 0.4800, 0.7200, 0.9000, 0.9400, 0.9400]),\n",
       " tensor([0.2400, 0.3200, 0.4600, 0.7000, 0.7400, 0.7600]),\n",
       " tensor([0.4200, 0.5800, 0.7200, 0.8800, 0.9400, 0.9400]),\n",
       " tensor([0.2600, 0.4400, 0.6200, 0.7600, 0.7800, 0.8000]),\n",
       " tensor([0.2200, 0.4200, 0.6200, 0.8600, 0.9000, 0.9200]),\n",
       " tensor([0.4000, 0.5000, 0.6200, 0.7400, 0.7600, 0.7800]),\n",
       " tensor([0.2600, 0.4000, 0.6200, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.2800, 0.5000, 0.6600, 0.8000, 0.8400, 0.8400]),\n",
       " tensor([0.5000, 0.5800, 0.6800, 0.8000, 0.8200, 0.8600]),\n",
       " tensor([0.2000, 0.2800, 0.5600, 0.8600, 0.9400, 0.9400]),\n",
       " tensor([0.2800, 0.4800, 0.6000, 0.8000, 0.8400, 0.8400]),\n",
       " tensor([0.2600, 0.3200, 0.4400, 0.7200, 0.8000, 0.8200]),\n",
       " tensor([0.2200, 0.4200, 0.6200, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.2800, 0.4000, 0.5800, 0.7800, 0.7800, 0.7800]),\n",
       " tensor([0.2800, 0.4000, 0.5200, 0.7800, 0.8200, 0.8400]),\n",
       " tensor([0.3600, 0.4200, 0.6600, 0.8400, 0.8600, 0.9000]),\n",
       " tensor([0.3800, 0.5200, 0.6200, 0.7600, 0.7600, 0.7600]),\n",
       " tensor([0.2800, 0.4400, 0.6600, 0.8800, 0.8800, 0.9400]),\n",
       " tensor([0.3200, 0.4200, 0.6400, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.2400, 0.3800, 0.6400, 0.7600, 0.7600, 0.7800]),\n",
       " tensor([0.2000, 0.3800, 0.5400, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.2800, 0.4600, 0.6200, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.2200, 0.4000, 0.5600, 0.7800, 0.8000, 0.8000]),\n",
       " tensor([0.2800, 0.4400, 0.7200, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.2000, 0.3800, 0.5800, 0.7400, 0.8000, 0.8400]),\n",
       " tensor([0.2400, 0.3600, 0.5200, 0.8000, 0.9000, 0.9200]),\n",
       " tensor([0.2800, 0.3600, 0.5400, 0.7600, 0.8000, 0.8400]),\n",
       " tensor([0.3000, 0.4600, 0.7600, 0.8400, 0.8400, 0.8600]),\n",
       " tensor([0.2600, 0.4400, 0.6600, 0.8600, 0.8600, 0.9000]),\n",
       " tensor([0.2200, 0.3200, 0.6200, 0.8200, 0.8600, 0.8600]),\n",
       " tensor([0.3200, 0.4000, 0.6400, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.2000, 0.3400, 0.6200, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.3200, 0.4200, 0.5400, 0.7600, 0.7600, 0.8400]),\n",
       " tensor([0.3000, 0.4800, 0.5400, 0.7000, 0.7400, 0.7800]),\n",
       " tensor([0.3400, 0.4800, 0.6800, 0.7800, 0.7800, 0.8000]),\n",
       " tensor([0.2600, 0.3400, 0.5000, 0.8200, 0.8400, 0.8800]),\n",
       " tensor([0.3800, 0.4800, 0.7400, 0.9200, 0.9200, 0.9200]),\n",
       " tensor([0.3200, 0.4000, 0.6000, 0.8600, 0.9000, 0.9200]),\n",
       " tensor([0.3000, 0.4400, 0.5800, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.3600, 0.4600, 0.5800, 0.7000, 0.7400, 0.8000]),\n",
       " tensor([0.2600, 0.4200, 0.5600, 0.7200, 0.8000, 0.8400]),\n",
       " tensor([0.2400, 0.4200, 0.6000, 0.8000, 0.8400, 0.9000]),\n",
       " tensor([0.1400, 0.3800, 0.5800, 0.7800, 0.8000, 0.8200]),\n",
       " tensor([0.2600, 0.3600, 0.6200, 0.9000, 0.9200, 0.9200]),\n",
       " tensor([0.3200, 0.4400, 0.6000, 0.7800, 0.8200, 0.8400]),\n",
       " tensor([0.2200, 0.4000, 0.6400, 0.8400, 0.9000, 0.9000]),\n",
       " tensor([0.2600, 0.4000, 0.6200, 0.7800, 0.8000, 0.8200]),\n",
       " tensor([0.3000, 0.5000, 0.6600, 0.8200, 0.8800, 0.9000]),\n",
       " tensor([0.2600, 0.4600, 0.6200, 0.7600, 0.7800, 0.8400]),\n",
       " tensor([0.3200, 0.4400, 0.6200, 0.8000, 0.8000, 0.8200]),\n",
       " tensor([0.3600, 0.5400, 0.7200, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.3200, 0.4200, 0.6600, 0.7000, 0.7000, 0.7400]),\n",
       " tensor([0.2400, 0.4400, 0.6400, 0.8200, 0.8600, 0.9000]),\n",
       " tensor([0.3800, 0.5400, 0.6200, 0.8600, 0.8600, 0.8600]),\n",
       " tensor([0.2200, 0.4400, 0.7200, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.2200, 0.3000, 0.5600, 0.8400, 0.8600, 0.9000]),\n",
       " tensor([0.2600, 0.4400, 0.6800, 0.8000, 0.8200, 0.8600]),\n",
       " tensor([0.3000, 0.4000, 0.6000, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.3200, 0.4600, 0.5400, 0.8200, 0.8800, 0.9400]),\n",
       " tensor([0.3200, 0.4200, 0.6000, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.2600, 0.3800, 0.6200, 0.7600, 0.7800, 0.8400]),\n",
       " tensor([0.3000, 0.4200, 0.5200, 0.7600, 0.7800, 0.7800]),\n",
       " tensor([0.2000, 0.4600, 0.6000, 0.8600, 0.9000, 0.9000]),\n",
       " tensor([0.2400, 0.4400, 0.6400, 0.9000, 0.9200, 0.9400]),\n",
       " tensor([0.2800, 0.4400, 0.6000, 0.7800, 0.8200, 0.8400]),\n",
       " tensor([0.2200, 0.4400, 0.6200, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.2400, 0.4000, 0.5800, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.2800, 0.4200, 0.5600, 0.7400, 0.7600, 0.7800]),\n",
       " tensor([0.3400, 0.4400, 0.6200, 0.8600, 0.8800, 0.9200]),\n",
       " tensor([0.2600, 0.4200, 0.6200, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.2000, 0.4400, 0.6400, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.3600, 0.5000, 0.7000, 0.8400, 0.8400, 0.8400]),\n",
       " tensor([0.3200, 0.5000, 0.6600, 0.9200, 0.9600, 0.9600]),\n",
       " tensor([0.2800, 0.5800, 0.7400, 0.8800, 0.8800, 0.8800]),\n",
       " tensor([0.2200, 0.3200, 0.5600, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.3000, 0.3600, 0.6400, 0.8600, 0.8600, 0.9000]),\n",
       " tensor([0.3600, 0.5000, 0.6600, 0.8600, 0.9000, 0.9000]),\n",
       " tensor([0.2600, 0.3800, 0.5200, 0.8000, 0.8000, 0.8800]),\n",
       " tensor([0.3000, 0.4400, 0.6800, 0.8400, 0.8400, 0.8400]),\n",
       " tensor([0.3400, 0.4200, 0.5600, 0.7400, 0.8200, 0.8800]),\n",
       " tensor([0.2600, 0.4600, 0.6400, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.2800, 0.3200, 0.5600, 0.8000, 0.8600, 0.8800]),\n",
       " tensor([0.2000, 0.4200, 0.6200, 0.8000, 0.8400, 0.8600]),\n",
       " tensor([0.3800, 0.5200, 0.6200, 0.8200, 0.8200, 0.8400]),\n",
       " tensor([0.2800, 0.4400, 0.5600, 0.7600, 0.8000, 0.8400]),\n",
       " tensor([0.1600, 0.3200, 0.5600, 0.8000, 0.8600, 0.8600]),\n",
       " tensor([0.2200, 0.3800, 0.5800, 0.7600, 0.8000, 0.8200]),\n",
       " tensor([0.2800, 0.4400, 0.6000, 0.7200, 0.7800, 0.8600]),\n",
       " tensor([0.2800, 0.4000, 0.5400, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.2000, 0.3200, 0.5200, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.2800, 0.4600, 0.6400, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.2400, 0.3400, 0.6400, 0.8600, 0.9400, 0.9400]),\n",
       " tensor([0.4400, 0.5400, 0.7200, 0.9200, 0.9600, 0.9600]),\n",
       " tensor([0.2600, 0.4400, 0.6400, 0.7600, 0.8000, 0.8400]),\n",
       " tensor([0.3200, 0.4200, 0.6200, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.3200, 0.3800, 0.5200, 0.7400, 0.8000, 0.8400]),\n",
       " tensor([0.2200, 0.4200, 0.5400, 0.8400, 0.8800, 0.9000]),\n",
       " tensor([0.3600, 0.4000, 0.6000, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.2400, 0.3000, 0.5600, 0.8200, 0.8400, 0.8600]),\n",
       " tensor([0.2200, 0.4600, 0.6600, 0.8800, 0.9000, 0.9400]),\n",
       " tensor([0.2600, 0.4200, 0.7400, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.1200, 0.4200, 0.6000, 0.8200, 0.8600, 0.8600]),\n",
       " tensor([0.2400, 0.4400, 0.6400, 0.8400, 0.8600, 0.8600]),\n",
       " tensor([0.4000, 0.4600, 0.6000, 0.7400, 0.7400, 0.7600]),\n",
       " tensor([0.3400, 0.4800, 0.6200, 0.8200, 0.8200, 0.8600]),\n",
       " tensor([0.2800, 0.4600, 0.6400, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.2200, 0.4000, 0.5600, 0.7800, 0.8000, 0.8400]),\n",
       " tensor([0.1200, 0.2600, 0.5800, 0.8400, 0.9000, 0.9000]),\n",
       " tensor([0.3200, 0.4200, 0.6200, 0.7800, 0.8400, 0.8600]),\n",
       " tensor([0.2600, 0.3800, 0.5600, 0.7600, 0.8000, 0.8200]),\n",
       " tensor([0.2800, 0.4600, 0.6400, 0.8200, 0.8600, 0.8800]),\n",
       " tensor([0.2600, 0.3800, 0.6600, 0.7800, 0.8000, 0.8200]),\n",
       " tensor([0.3400, 0.4400, 0.6600, 0.8800, 0.8800, 0.8800]),\n",
       " tensor([0.2800, 0.4600, 0.6000, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.3000, 0.4400, 0.5400, 0.7400, 0.8000, 0.8200]),\n",
       " tensor([0.4200, 0.6600, 0.7200, 0.8000, 0.8400, 0.8600]),\n",
       " tensor([0.3200, 0.4000, 0.6800, 0.8800, 0.9000, 0.9200]),\n",
       " tensor([0.2400, 0.4200, 0.6200, 0.6800, 0.7600, 0.7600]),\n",
       " tensor([0.2800, 0.4800, 0.6000, 0.8000, 0.8200, 0.8200]),\n",
       " tensor([0.1200, 0.2400, 0.4200, 0.7600, 0.7800, 0.8200]),\n",
       " tensor([0.2800, 0.3800, 0.6400, 0.8000, 0.8000, 0.8000]),\n",
       " tensor([0.2800, 0.4200, 0.5600, 0.7000, 0.7200, 0.7800]),\n",
       " tensor([0.3600, 0.4800, 0.6800, 0.8600, 0.8800, 0.8800]),\n",
       " tensor([0.2200, 0.3400, 0.6600, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.3000, 0.5000, 0.6400, 0.8800, 0.9000, 0.9600]),\n",
       " tensor([0.2600, 0.4200, 0.5800, 0.7800, 0.8400, 0.9000]),\n",
       " tensor([0.3600, 0.5400, 0.6200, 0.8400, 0.8800, 0.9200]),\n",
       " tensor([0.2600, 0.4000, 0.6600, 0.8000, 0.8600, 0.8600]),\n",
       " tensor([0.1800, 0.3200, 0.6200, 0.8800, 0.8800, 0.9400]),\n",
       " tensor([0.4000, 0.4800, 0.7200, 0.8000, 0.8400, 0.8600]),\n",
       " tensor([0.1800, 0.3000, 0.5400, 0.7800, 0.8000, 0.8200]),\n",
       " tensor([0.3200, 0.4400, 0.5600, 0.7800, 0.8200, 0.8600]),\n",
       " tensor([0.2400, 0.3600, 0.6000, 0.8400, 0.8600, 0.9200]),\n",
       " tensor([0.2400, 0.3200, 0.5400, 0.8000, 0.8000, 0.8400]),\n",
       " tensor([0.2800, 0.5000, 0.6200, 0.8600, 0.8800, 0.9000]),\n",
       " tensor([0.2400, 0.4000, 0.7200, 0.8000, 0.8000, 0.8200]),\n",
       " tensor([0.2000, 0.3400, 0.5600, 0.7600, 0.8200, 0.9000]),\n",
       " tensor([0.3800, 0.4400, 0.5800, 0.8000, 0.8000, 0.8200]),\n",
       " tensor([0.3200, 0.4800, 0.6200, 0.7800, 0.7800, 0.8000]),\n",
       " tensor([0.3200, 0.4800, 0.7200, 0.8400, 0.8600, 0.8800]),\n",
       " tensor([0.2000, 0.2800, 0.5600, 0.8000, 0.8400, 0.8800]),\n",
       " tensor([0.1200, 0.3400, 0.6600, 0.8200, 0.8400, 0.9000]),\n",
       " tensor([0.1200, 0.2800, 0.5000, 0.6800, 0.7000, 0.7600]),\n",
       " tensor([0.2200, 0.3800, 0.6000, 0.8000, 0.8800, 0.9200]),\n",
       " tensor([0.2400, 0.4200, 0.6000, 0.7800, 0.8000, 0.8000]),\n",
       " tensor([0.1800, 0.2800, 0.5000, 0.8200, 0.8600, 0.8600]),\n",
       " tensor([0.3600, 0.4800, 0.6200, 0.7800, 0.8400, 0.8800]),\n",
       " tensor([0.2000, 0.4200, 0.6600, 0.8400, 0.8800, 0.9200]),\n",
       " tensor([0.2400, 0.3600, 0.5800, 0.9000, 0.9000, 0.9200]),\n",
       " tensor([0.1600, 0.3000, 0.5800, 0.8000, 0.8600, 0.8800]),\n",
       " tensor([0.2400, 0.4200, 0.6200, 0.8000, 0.8000, 0.8400]),\n",
       " tensor([0.3200, 0.4600, 0.6600, 0.8600, 0.9000, 0.9000]),\n",
       " tensor([0.3600, 0.5200, 0.5400, 0.8600, 0.8600, 0.8600]),\n",
       " tensor([0.3000, 0.4800, 0.7200, 0.8600, 0.8600, 0.8600]),\n",
       " tensor([0.3000, 0.3600, 0.4600, 0.6600, 0.7200, 0.7800]),\n",
       " tensor([0.3000, 0.3600, 0.6600, 0.8000, 0.8600, 0.9200]),\n",
       " tensor([0.2200, 0.4400, 0.6600, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.2400, 0.4000, 0.6200, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.4000, 0.5400, 0.7200, 0.8800, 0.9000, 0.9400]),\n",
       " tensor([0.2400, 0.3600, 0.6000, 0.8200, 0.8600, 0.9000]),\n",
       " tensor([0.1800, 0.3800, 0.5600, 0.9000, 0.9200, 0.9400]),\n",
       " tensor([0.1400, 0.3000, 0.5600, 0.8800, 0.8800, 0.9000]),\n",
       " tensor([0.3200, 0.4600, 0.6400, 0.7800, 0.7800, 0.8000]),\n",
       " tensor([0.2600, 0.3400, 0.5400, 0.8000, 0.8200, 0.8400]),\n",
       " tensor([0.3400, 0.4200, 0.6600, 0.8000, 0.8200, 0.9000]),\n",
       " tensor([0.3200, 0.4400, 0.5600, 0.6800, 0.7000, 0.7800]),\n",
       " tensor([0.2600, 0.4400, 0.6400, 0.8800, 0.9000, 0.9400]),\n",
       " tensor([0.2000, 0.3200, 0.5600, 0.8000, 0.8200, 0.8600]),\n",
       " tensor([0.3000, 0.4600, 0.6200, 0.8400, 0.8800, 0.8800]),\n",
       " tensor([0.2200, 0.4000, 0.5800, 0.8400, 0.8800, 0.9000]),\n",
       " tensor([0.2000, 0.4400, 0.5800, 0.8600, 0.9000, 0.9400]),\n",
       " tensor([0.3000, 0.4200, 0.6000, 0.7200, 0.7400, 0.7400]),\n",
       " tensor([0.2000, 0.4200, 0.5600, 0.7400, 0.7800, 0.8200]),\n",
       " tensor([0.4000, 0.5200, 0.6400, 0.9000, 0.9200, 0.9200])]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_recall_at_k = [[],[],[],[],[],[]]\n",
    "for a in recall_values:\n",
    "    mean_recall_at_k[0].append(a[0])\n",
    "    mean_recall_at_k[1].append(a[1])\n",
    "    mean_recall_at_k[2].append(a[2])\n",
    "    mean_recall_at_k[3].append(a[3])\n",
    "    mean_recall_at_k[4].append(a[4])\n",
    "    mean_recall_at_k[5].append(a[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27323332\n",
      "0.41893333\n",
      "0.60836667\n",
      "0.8074\n",
      "0.83940005\n",
      "0.8641334\n"
     ]
    }
   ],
   "source": [
    "for i in mean_recall_at_k:\n",
    "    print(np.mean(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Precision at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Precision at k\n",
    "    \"\"\"\n",
    "    intersection = np.intersect1d(y_true, y_pred[:k])\n",
    "    return len(intersection) / k\n",
    "\n",
    "def rel_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Relevance at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Relevance at k\n",
    "    \"\"\"\n",
    "    if y_pred[k-1] in y_true:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def average_precision_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Average Precision at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Average Precision at k\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    rel_counter = 0\n",
    "    for i in range(1, k+1):\n",
    "        ap += precision_at_k(y_true, y_pred, i) * rel_at_k(y_true, y_pred, i)\n",
    "        rel_counter += rel_at_k(y_true, y_pred, i)\n",
    "    #return ap / min(k, len(y_true))\n",
    "    return ap / rel_counter\n",
    "\n",
    "\n",
    "def mean_average_precision(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes MAP at k\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            2D Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            2D Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           MAP at k\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean([average_precision_at_k(gt, pred, k) \\\n",
    "                    for gt, pred in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP_at_k(Fvec, imgLab,gt,rank=1):\n",
    "    # imgLab numpy.ndarray of shape: (8041,)\n",
    "   \n",
    "    N = len(imgLab) #8041 labels\n",
    "\n",
    "    imgLab = torch.LongTensor([imgLab[i] for i in range(len(imgLab))])\n",
    "    # imgLab.shape: [8041]\n",
    "    # Fvec.shape: [8041, 128]\n",
    "    \n",
    "    D = Fvec.mm(torch.t(Fvec)) # mm: matrix multiplication. (n×m) mm (m×p) results in  (n×p) tensor.\n",
    "    # [8041, 128] mm [128, 8041] --> [8041, 8041] this is D matrix\n",
    "    # There are 1's along the diagonal!\n",
    "    \n",
    "    D[torch.eye(len(imgLab)).bool()] = -1 \n",
    "    # torch.eye: Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "    # D[torch.eye(len(imgLab)).bool()]: diagonal elements of D will take a value of -1 ; the rest will remain the same\n",
    "    \n",
    "    #print(\"Distance Matris Shape\" , D.shape)\n",
    "    print(D)\n",
    "    _,idx = D.topk(rank[-1])\n",
    "\n",
    "\n",
    "    return mean_average_precision(np.array(gt),idx.numpy(), k= rank[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i.th element 0  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 1  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 2  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 3  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 4  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 5  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 6  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 7  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 8  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 9  floor is  0 max is  10 floor * 10 =  0\n",
      "for i.th element 10  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 11  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 12  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 13  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 14  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 15  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 16  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 17  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 18  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 19  floor is  1 max is  20 floor * 10 =  10\n",
      "for i.th element 20  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 21  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 22  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 23  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 24  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 25  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 26  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 27  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 28  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 29  floor is  2 max is  30 floor * 10 =  20\n",
      "for i.th element 30  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 31  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 32  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 33  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 34  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 35  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 36  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 37  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 38  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 39  floor is  3 max is  40 floor * 10 =  30\n",
      "for i.th element 40  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 41  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 42  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 43  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 44  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 45  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 46  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 47  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 48  floor is  4 max is  50 floor * 10 =  40\n",
      "for i.th element 49  floor is  4 max is  50 floor * 10 =  40\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "y_true = []\n",
    "for i in  range(50):\n",
    "    floor = math.floor(i/10) \n",
    "    max = ((floor + 1 ) * 10 ) \n",
    "\n",
    "    print(\"for i.th element\" , str(i) , \" floor is \" , floor , \"max is \", max , \"floor * 10 = \" , floor*10 )\n",
    "    range_list = list(range(floor*10 , max))\n",
    "    range_list.remove(i)\n",
    "    y_true.append(range_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       " [11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [10, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [10, 11, 13, 14, 15, 16, 17, 18, 19],\n",
       " [10, 11, 12, 14, 15, 16, 17, 18, 19],\n",
       " [10, 11, 12, 13, 15, 16, 17, 18, 19],\n",
       " [10, 11, 12, 13, 14, 16, 17, 18, 19],\n",
       " [10, 11, 12, 13, 14, 15, 17, 18, 19],\n",
       " [10, 11, 12, 13, 14, 15, 16, 18, 19],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 19],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
       " [21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       " [20, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       " [20, 21, 23, 24, 25, 26, 27, 28, 29],\n",
       " [20, 21, 22, 24, 25, 26, 27, 28, 29],\n",
       " [20, 21, 22, 23, 25, 26, 27, 28, 29],\n",
       " [20, 21, 22, 23, 24, 26, 27, 28, 29],\n",
       " [20, 21, 22, 23, 24, 25, 27, 28, 29],\n",
       " [20, 21, 22, 23, 24, 25, 26, 28, 29],\n",
       " [20, 21, 22, 23, 24, 25, 26, 27, 29],\n",
       " [20, 21, 22, 23, 24, 25, 26, 27, 28],\n",
       " [31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       " [30, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       " [30, 31, 33, 34, 35, 36, 37, 38, 39],\n",
       " [30, 31, 32, 34, 35, 36, 37, 38, 39],\n",
       " [30, 31, 32, 33, 35, 36, 37, 38, 39],\n",
       " [30, 31, 32, 33, 34, 36, 37, 38, 39],\n",
       " [30, 31, 32, 33, 34, 35, 37, 38, 39],\n",
       " [30, 31, 32, 33, 34, 35, 36, 38, 39],\n",
       " [30, 31, 32, 33, 34, 35, 36, 37, 39],\n",
       " [30, 31, 32, 33, 34, 35, 36, 37, 38],\n",
       " [41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       " [40, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       " [40, 41, 43, 44, 45, 46, 47, 48, 49],\n",
       " [40, 41, 42, 44, 45, 46, 47, 48, 49],\n",
       " [40, 41, 42, 43, 45, 46, 47, 48, 49],\n",
       " [40, 41, 42, 43, 44, 46, 47, 48, 49],\n",
       " [40, 41, 42, 43, 44, 45, 47, 48, 49],\n",
       " [40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
       " [40, 41, 42, 43, 44, 45, 46, 47, 49],\n",
       " [40, 41, 42, 43, 44, 45, 46, 47, 48]]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n"
     ]
    }
   ],
   "source": [
    "mean_map = []\n",
    "for i in range(600):\n",
    "    print(i)\n",
    "    a = triplet_imagenet_dataset.sample(\"test\",mode=\"img_retrieval\",samples_per_class = 10)\n",
    "    opt.zero_grad() # for each batch, gradients should be cleaned.\n",
    "    # Compute meta-training loss\n",
    "    # print('Task no: ', task)\n",
    "    learner = maml.clone()\n",
    "    batch = a\n",
    "\n",
    "    train_error, _ = fast_adapt_image_retrieval(batch,\n",
    "                                                        learner,\n",
    "                                                        combined_loss_fn,\n",
    "                                                        adaptation_steps,\n",
    "                                                        shots,\n",
    "                                                        ways,\n",
    "                                                        device)                                                \n",
    "    embeddings = []\n",
    "    labels = torch.from_numpy(np.array(list(a[3])))\n",
    "    for idx,image in enumerate(a[2]):\n",
    "        embedding,class_prob =learner.forward_once(torch.unsqueeze(image,0).to(device)) \n",
    "        embedding = F.normalize(embedding, p = 2 , dim =1).cpu()\n",
    "        embeddings.append(np.array(embedding[0].tolist()))\n",
    "    embeddings = torch.from_numpy(np.array(embeddings))\n",
    "\n",
    "    mean_map.append(mAP_at_k(embeddings,labels,y_true,[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11892957818930043"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08166754850088184,\n",
       " 0.14786243386243386,\n",
       " 0.13549647266313933,\n",
       " 0.12651675485008818,\n",
       " 0.12207936507936505,\n",
       " 0.1723809523809524,\n",
       " 0.07552469135802468,\n",
       " 0.10566754850088184,\n",
       " 0.11682892416225751,\n",
       " 0.16264814814814815,\n",
       " 0.13576543209876543,\n",
       " 0.10209347442680775,\n",
       " 0.09438007054673721,\n",
       " 0.11770899470899471,\n",
       " 0.10703262786596122,\n",
       " 0.12149029982363314,\n",
       " 0.1182689594356261,\n",
       " 0.13524603174603175,\n",
       " 0.11179894179894179,\n",
       " 0.08317460317460316,\n",
       " 0.09112698412698414,\n",
       " 0.09719664902998236,\n",
       " 0.11578395061728396,\n",
       " 0.10047971781305114,\n",
       " 0.1503439153439153,\n",
       " 0.13501940035273366,\n",
       " 0.12100088183421515,\n",
       " 0.1255679012345679,\n",
       " 0.15276719576719575,\n",
       " 0.11666931216931217,\n",
       " 0.16171869488536153,\n",
       " 0.09003174603174602,\n",
       " 0.09621252204585538,\n",
       " 0.14277072310405642,\n",
       " 0.12371252204585537,\n",
       " 0.1158174603174603,\n",
       " 0.10629100529100528,\n",
       " 0.137347442680776,\n",
       " 0.11038447971781302,\n",
       " 0.12893562610229278,\n",
       " 0.1252821869488536,\n",
       " 0.11294797178130513,\n",
       " 0.11006878306878307,\n",
       " 0.08047354497354497,\n",
       " 0.14390740740740743,\n",
       " 0.12644708994708995,\n",
       " 0.13011904761904763,\n",
       " 0.09384303350970016,\n",
       " 0.13645061728395064,\n",
       " 0.1195352733686067,\n",
       " 0.13520017636684303,\n",
       " 0.13038447971781303,\n",
       " 0.12084391534391535,\n",
       " 0.06937037037037036,\n",
       " 0.13594356261022925,\n",
       " 0.15258112874779542,\n",
       " 0.06687918871252203,\n",
       " 0.08149118165784833,\n",
       " 0.1237010582010582,\n",
       " 0.12940299823633156,\n",
       " 0.11828130511463844,\n",
       " 0.11361904761904762,\n",
       " 0.11701322751322751,\n",
       " 0.09488271604938273,\n",
       " 0.08944973544973545,\n",
       " 0.11937125220458553,\n",
       " 0.09613844797178128,\n",
       " 0.11850881834215167,\n",
       " 0.08470546737213404,\n",
       " 0.08938536155202823,\n",
       " 0.11766402116402118,\n",
       " 0.13670370370370372,\n",
       " 0.13855291005291004,\n",
       " 0.08854144620811287,\n",
       " 0.14276807760141094,\n",
       " 0.12068253968253968,\n",
       " 0.1388677248677249,\n",
       " 0.136157848324515,\n",
       " 0.11111640211640211,\n",
       " 0.09675044091710759,\n",
       " 0.13627865961199292,\n",
       " 0.16474867724867728,\n",
       " 0.13629453262786595,\n",
       " 0.10362081128747797,\n",
       " 0.1405520282186949,\n",
       " 0.11018253968253967,\n",
       " 0.1326490299823633,\n",
       " 0.14332186948853615,\n",
       " 0.15743915343915343,\n",
       " 0.12935714285714284,\n",
       " 0.13959259259259257,\n",
       " 0.11250264550264552,\n",
       " 0.07816137566137565,\n",
       " 0.0801111111111111,\n",
       " 0.12817989417989417,\n",
       " 0.10286507936507937,\n",
       " 0.10732980599647267,\n",
       " 0.10097266313932979,\n",
       " 0.13391005291005292,\n",
       " 0.14494268077601408,\n",
       " 0.09325925925925926,\n",
       " 0.10346913580246912,\n",
       " 0.12859171075837741,\n",
       " 0.1157142857142857,\n",
       " 0.09382804232804233,\n",
       " 0.12711199294532627,\n",
       " 0.10208465608465607,\n",
       " 0.1567962962962963,\n",
       " 0.1360246913580247,\n",
       " 0.10687389770723103,\n",
       " 0.11669753086419751,\n",
       " 0.14093386243386244,\n",
       " 0.1142416225749559,\n",
       " 0.09922663139329804,\n",
       " 0.0941225749559083,\n",
       " 0.1143703703703704,\n",
       " 0.1093941798941799,\n",
       " 0.12186067019400351,\n",
       " 0.10168606701940036,\n",
       " 0.10062610229276896,\n",
       " 0.08940035273368606,\n",
       " 0.12512345679012346,\n",
       " 0.12528306878306877,\n",
       " 0.1393721340388007,\n",
       " 0.12490740740740741,\n",
       " 0.11881040564373897,\n",
       " 0.10482980599647265,\n",
       " 0.08037742504409172,\n",
       " 0.14670987654320988,\n",
       " 0.11449559082892416,\n",
       " 0.06765520282186949,\n",
       " 0.1282010582010582,\n",
       " 0.12373721340388007,\n",
       " 0.11232716049382717,\n",
       " 0.16394973544973543,\n",
       " 0.1112768959435626,\n",
       " 0.14738888888888887,\n",
       " 0.09530423280423278,\n",
       " 0.20531922398589061,\n",
       " 0.09893033509700175,\n",
       " 0.1398007054673721,\n",
       " 0.09014726631393298,\n",
       " 0.11989858906525573,\n",
       " 0.10676190476190474,\n",
       " 0.11219047619047619,\n",
       " 0.11679541446208114,\n",
       " 0.1410970017636684,\n",
       " 0.11222663139329805,\n",
       " 0.13603880070546737,\n",
       " 0.09032451499118166,\n",
       " 0.13478924162257497,\n",
       " 0.07758377425044091,\n",
       " 0.14537389770723105,\n",
       " 0.08544268077601411,\n",
       " 0.10709435626102293,\n",
       " 0.06746031746031746,\n",
       " 0.088494708994709,\n",
       " 0.15548677248677248,\n",
       " 0.13338624338624339,\n",
       " 0.14026014109347443,\n",
       " 0.10167548500881834,\n",
       " 0.11854409171075836,\n",
       " 0.12754938271604938,\n",
       " 0.14590388007054672,\n",
       " 0.07454497354497354,\n",
       " 0.13529629629629628,\n",
       " 0.13335185185185183,\n",
       " 0.12156172839506171,\n",
       " 0.11959700176366843,\n",
       " 0.12078571428571429,\n",
       " 0.11689506172839505,\n",
       " 0.09551587301587303,\n",
       " 0.11464550264550263,\n",
       " 0.11565343915343915,\n",
       " 0.07642504409171076,\n",
       " 0.18416049382716046,\n",
       " 0.10295943562610228,\n",
       " 0.14378835978835977,\n",
       " 0.11826543209876542,\n",
       " 0.09953439153439154,\n",
       " 0.11421252204585537,\n",
       " 0.12748148148148147,\n",
       " 0.11091798941798943,\n",
       " 0.14374867724867724,\n",
       " 0.09852116402116401,\n",
       " 0.13341887125220458,\n",
       " 0.1347663139329806,\n",
       " 0.12032451499118166,\n",
       " 0.1328641975308642,\n",
       " 0.0729779541446208,\n",
       " 0.12039506172839505,\n",
       " 0.08069135802469135,\n",
       " 0.12172045855379186,\n",
       " 0.13086243386243387,\n",
       " 0.13027336860670194,\n",
       " 0.12059347442680775,\n",
       " 0.14500440917107582,\n",
       " 0.11744268077601411,\n",
       " 0.08084479717813052,\n",
       " 0.10682186948853614,\n",
       " 0.1638562610229277,\n",
       " 0.15238447971781302,\n",
       " 0.10103439153439153,\n",
       " 0.16296384479717813,\n",
       " 0.16258994708994706,\n",
       " 0.1017742504409171,\n",
       " 0.13617283950617282,\n",
       " 0.1017689594356261,\n",
       " 0.14163403880070546,\n",
       " 0.1135079365079365,\n",
       " 0.09566666666666666,\n",
       " 0.1269726631393298,\n",
       " 0.11837125220458553,\n",
       " 0.0812522045855379,\n",
       " 0.1168968253968254,\n",
       " 0.10746031746031745,\n",
       " 0.10003880070546738,\n",
       " 0.1193289241622575,\n",
       " 0.10032980599647265,\n",
       " 0.13883597883597884,\n",
       " 0.13850617283950617,\n",
       " 0.1399700176366843,\n",
       " 0.09700970017636684,\n",
       " 0.09642504409171075,\n",
       " 0.1490864197530864,\n",
       " 0.11921604938271603,\n",
       " 0.1129532627865961,\n",
       " 0.09475661375661376,\n",
       " 0.10298941798941799,\n",
       " 0.10180952380952381,\n",
       " 0.1270511463844797,\n",
       " 0.132157848324515,\n",
       " 0.1332010582010582,\n",
       " 0.09968518518518517,\n",
       " 0.16922663139329808,\n",
       " 0.09713139329805998,\n",
       " 0.09841887125220458,\n",
       " 0.11434832451499119,\n",
       " 0.1681649029982363,\n",
       " 0.10391181657848322,\n",
       " 0.08555643738977071,\n",
       " 0.09336596119929454,\n",
       " 0.13334391534391535,\n",
       " 0.09356790123456787,\n",
       " 0.09709347442680775,\n",
       " 0.1014215167548501,\n",
       " 0.1431172839506173,\n",
       " 0.14102028218694887,\n",
       " 0.12656878306878305,\n",
       " 0.09892416225749559,\n",
       " 0.1391604938271605,\n",
       " 0.11892504409171074,\n",
       " 0.14905114638447972,\n",
       " 0.09581305114638447,\n",
       " 0.09997619047619047,\n",
       " 0.13458201058201058,\n",
       " 0.10975661375661375,\n",
       " 0.10523280423280423,\n",
       " 0.12283862433862433,\n",
       " 0.12980776014109346,\n",
       " 0.10427248677248677,\n",
       " 0.13718606701940037,\n",
       " 0.10290123456790123,\n",
       " 0.11181922398589064,\n",
       " 0.09149735449735448,\n",
       " 0.10479717813051145,\n",
       " 0.11069753086419752,\n",
       " 0.13608377425044094,\n",
       " 0.08625220458553791,\n",
       " 0.14498589065255732,\n",
       " 0.14672751322751323,\n",
       " 0.08488977072310405,\n",
       " 0.15442504409171076,\n",
       " 0.13066402116402118,\n",
       " 0.1256710758377425,\n",
       " 0.1002848324514991,\n",
       " 0.10311816578483246,\n",
       " 0.1302557319223986,\n",
       " 0.1076446208112875,\n",
       " 0.17572839506172838,\n",
       " 0.15092680776014109,\n",
       " 0.08045502645502646,\n",
       " 0.12351322751322752,\n",
       " 0.09306966490299823,\n",
       " 0.1324206349206349,\n",
       " 0.16908641975308641,\n",
       " 0.1462151675485009,\n",
       " 0.0937989417989418,\n",
       " 0.07203174603174603,\n",
       " 0.08304144620811288,\n",
       " 0.18336243386243387,\n",
       " 0.1362821869488536,\n",
       " 0.13105291005291003,\n",
       " 0.0956031746031746,\n",
       " 0.15020282186948852,\n",
       " 0.13765961199294532,\n",
       " 0.1144188712522046,\n",
       " 0.12131128747795411,\n",
       " 0.13054761904761902,\n",
       " 0.10330246913580247,\n",
       " 0.10494708994708996,\n",
       " 0.13757936507936508,\n",
       " 0.08104673721340389,\n",
       " 0.08371075837742505,\n",
       " 0.09433862433862433,\n",
       " 0.16015608465608466,\n",
       " 0.13242857142857142,\n",
       " 0.07850617283950617,\n",
       " 0.15253174603174602,\n",
       " 0.09458112874779541,\n",
       " 0.15605202821869488,\n",
       " 0.08687742504409171,\n",
       " 0.11474867724867725,\n",
       " 0.1582433862433862,\n",
       " 0.10894620811287478,\n",
       " 0.10361287477954145,\n",
       " 0.15755996472663136,\n",
       " 0.09814285714285713,\n",
       " 0.1728518518518519,\n",
       " 0.10940828924162257,\n",
       " 0.11980335097001765,\n",
       " 0.08690035273368607,\n",
       " 0.2068174603174603,\n",
       " 0.10347442680776013,\n",
       " 0.10587830687830686,\n",
       " 0.08864638447971782,\n",
       " 0.08816490299823633,\n",
       " 0.10847795414462079,\n",
       " 0.16046560846560848,\n",
       " 0.09991005291005292,\n",
       " 0.1500626102292769,\n",
       " 0.09706878306878305,\n",
       " 0.143,\n",
       " 0.1388853615520282,\n",
       " 0.12318077601410934,\n",
       " 0.10160141093474426,\n",
       " 0.09244179894179894,\n",
       " 0.13898236331569663,\n",
       " 0.11466402116402116,\n",
       " 0.11810493827160494,\n",
       " 0.11396296296296297,\n",
       " 0.14303086419753086,\n",
       " 0.11131128747795413,\n",
       " 0.09294003527336861,\n",
       " 0.10117283950617283,\n",
       " 0.09506790123456792,\n",
       " 0.08606878306878307,\n",
       " 0.12121516754850087,\n",
       " 0.10169753086419753,\n",
       " 0.11926631393298061,\n",
       " 0.10255467372134039,\n",
       " 0.13644091710758377,\n",
       " 0.11942592592592591,\n",
       " 0.10443474426807758,\n",
       " 0.14417107583774247,\n",
       " 0.10782010582010582,\n",
       " 0.1280811287477954,\n",
       " 0.10829629629629631,\n",
       " 0.12671340388007052,\n",
       " 0.1217389770723104,\n",
       " 0.13276278659611992,\n",
       " 0.09568342151675484,\n",
       " 0.1322063492063492,\n",
       " 0.13186772486772486,\n",
       " 0.15457671957671956,\n",
       " 0.102847442680776,\n",
       " 0.13701234567901233,\n",
       " 0.12432010582010582,\n",
       " 0.16117107583774246,\n",
       " 0.10742857142857141,\n",
       " 0.09534391534391533,\n",
       " 0.08320458553791887,\n",
       " 0.12067460317460318,\n",
       " 0.12451675485008819,\n",
       " 0.13922663139329805,\n",
       " 0.13633597883597884,\n",
       " 0.12100088183421516,\n",
       " 0.12517195767195766,\n",
       " 0.1585273368606702,\n",
       " 0.09091093474426808,\n",
       " 0.08914021164021163,\n",
       " 0.1521552028218695,\n",
       " 0.15038007054673722,\n",
       " 0.11986155202821872,\n",
       " 0.09788977072310404,\n",
       " 0.11655291005291005,\n",
       " 0.12232980599647268,\n",
       " 0.11305379188712522,\n",
       " 0.16551940035273366,\n",
       " 0.11197619047619048,\n",
       " 0.14058553791887124,\n",
       " 0.08728571428571427,\n",
       " 0.15088007054673722,\n",
       " 0.16000881834215164,\n",
       " 0.14332804232804233,\n",
       " 0.1231878306878307,\n",
       " 0.1426710758377425,\n",
       " 0.10384832451499117,\n",
       " 0.0788694885361552,\n",
       " 0.1145432098765432,\n",
       " 0.09621340388007056,\n",
       " 0.08635273368606702,\n",
       " 0.10851499118165783,\n",
       " 0.11740476190476189,\n",
       " 0.12244885361552027,\n",
       " 0.13877248677248677,\n",
       " 0.11789682539682537,\n",
       " 0.12036243386243387,\n",
       " 0.11514109347442679,\n",
       " 0.0854620811287478,\n",
       " 0.09618959435626101,\n",
       " 0.09748765432098765,\n",
       " 0.10124603174603175,\n",
       " 0.14786331569664904,\n",
       " 0.12026984126984125,\n",
       " 0.18113492063492065,\n",
       " 0.09903262786596122,\n",
       " 0.12427601410934741,\n",
       " 0.09244268077601411,\n",
       " 0.1006710758377425,\n",
       " 0.11160934744268078,\n",
       " 0.11653086419753085,\n",
       " 0.08554850088183422,\n",
       " 0.11025837742504409,\n",
       " 0.0951657848324515,\n",
       " 0.11570899470899469,\n",
       " 0.08198236331569664,\n",
       " 0.16444973544973543,\n",
       " 0.14280952380952378,\n",
       " 0.10131569664902998,\n",
       " 0.13797883597883598,\n",
       " 0.11085537918871252,\n",
       " 0.09321781305114638,\n",
       " 0.09803703703703703,\n",
       " 0.12728395061728393,\n",
       " 0.11273280423280423,\n",
       " 0.09304497354497356,\n",
       " 0.11646472663139329,\n",
       " 0.09165784832451498,\n",
       " 0.11582010582010581,\n",
       " 0.11644620811287477,\n",
       " 0.1930925925925926,\n",
       " 0.09459876543209876,\n",
       " 0.1497363315696649,\n",
       " 0.10132275132275131,\n",
       " 0.12835361552028218,\n",
       " 0.12412786596119929,\n",
       " 0.1455467372134039,\n",
       " 0.08003439153439156,\n",
       " 0.12036331569664903,\n",
       " 0.13051763668430336,\n",
       " 0.11202292768959435,\n",
       " 0.14510229276895942,\n",
       " 0.10984391534391534,\n",
       " 0.16407231040564374,\n",
       " 0.1730679012345679,\n",
       " 0.13793298059964726,\n",
       " 0.09328395061728394,\n",
       " 0.1319779541446208,\n",
       " 0.11359435626102293,\n",
       " 0.15699823633156967,\n",
       " 0.11528306878306877,\n",
       " 0.11313492063492062,\n",
       " 0.10343738977072311,\n",
       " 0.14006613756613756,\n",
       " 0.14146296296296296,\n",
       " 0.1183015873015873,\n",
       " 0.09948324514991182,\n",
       " 0.11676807760141093,\n",
       " 0.16792328042328042,\n",
       " 0.10798148148148147,\n",
       " 0.10045061728395062,\n",
       " 0.16352469135802467,\n",
       " 0.10276543209876543,\n",
       " 0.12873721340388009,\n",
       " 0.14808818342151675,\n",
       " 0.12548412698412698,\n",
       " 0.07355996472663139,\n",
       " 0.09217548500881832,\n",
       " 0.08623104056437389,\n",
       " 0.1131984126984127,\n",
       " 0.12967813051146385,\n",
       " 0.08694797178130509,\n",
       " 0.0934373897707231,\n",
       " 0.09952557319223984,\n",
       " 0.09895855379188712,\n",
       " 0.11586948853615521,\n",
       " 0.11597089947089946,\n",
       " 0.10044708994708994,\n",
       " 0.07312433862433862,\n",
       " 0.09056966490299825,\n",
       " 0.1155238095238095,\n",
       " 0.1269091710758377,\n",
       " 0.1083201058201058,\n",
       " 0.13064991181657848,\n",
       " 0.12583509700176368,\n",
       " 0.14843386243386242,\n",
       " 0.1295537918871252,\n",
       " 0.14977601410934743,\n",
       " 0.13264285714285715,\n",
       " 0.11463227513227511,\n",
       " 0.10521693121693121,\n",
       " 0.11250352733686068,\n",
       " 0.12723544973544973,\n",
       " 0.11765079365079366,\n",
       " 0.09032275132275133,\n",
       " 0.17870370370370373,\n",
       " 0.12722486772486774,\n",
       " 0.0978721340388007,\n",
       " 0.17294708994709,\n",
       " 0.18016931216931215,\n",
       " 0.10355643738977072,\n",
       " 0.16054320987654322,\n",
       " 0.11833597883597885,\n",
       " 0.12168871252204586,\n",
       " 0.13750088183421513,\n",
       " 0.12040828924162258,\n",
       " 0.12119135802469136,\n",
       " 0.17051322751322753,\n",
       " 0.08488007054673721,\n",
       " 0.12387037037037035,\n",
       " 0.15364726631393297,\n",
       " 0.1313880070546737,\n",
       " 0.10678924162257494,\n",
       " 0.10236243386243386,\n",
       " 0.10072751322751321,\n",
       " 0.16738712522045854,\n",
       " 0.10456790123456791,\n",
       " 0.06746031746031746,\n",
       " 0.11431040564373897,\n",
       " 0.13051234567901235,\n",
       " 0.09952380952380951,\n",
       " 0.10552292768959434,\n",
       " 0.1469647266313933,\n",
       " 0.1578668430335097,\n",
       " 0.10728395061728394,\n",
       " 0.11668165784832452,\n",
       " 0.08230599647266315,\n",
       " 0.09167548500881834,\n",
       " 0.14512345679012348,\n",
       " 0.13950264550264546,\n",
       " 0.11664638447971781,\n",
       " 0.09080335097001763,\n",
       " 0.11102204585537917,\n",
       " 0.12115432098765433,\n",
       " 0.08496119929453261,\n",
       " 0.11390388007054673,\n",
       " 0.08674779541446208,\n",
       " 0.11630952380952381,\n",
       " 0.1132116402116402,\n",
       " 0.12551499118165785,\n",
       " 0.12521516754850087,\n",
       " 0.10527865961199295,\n",
       " 0.1267336860670194,\n",
       " 0.15167989417989416,\n",
       " 0.11555114638447972,\n",
       " 0.09309611992945324,\n",
       " 0.1543509700176367,\n",
       " 0.11145590828924162,\n",
       " 0.12427689594356259,\n",
       " 0.08638007054673721,\n",
       " 0.12382539682539682,\n",
       " 0.11927248677248677,\n",
       " 0.0962142857142857,\n",
       " 0.12171164021164022,\n",
       " 0.14727865961199296,\n",
       " 0.11915873015873016,\n",
       " 0.15029365079365078,\n",
       " 0.13341975308641973,\n",
       " 0.13608730158730156,\n",
       " 0.0981164021164021,\n",
       " 0.12187301587301587,\n",
       " 0.0987601410934744,\n",
       " 0.11906966490299821,\n",
       " 0.13339858906525573,\n",
       " 0.13793386243386244,\n",
       " 0.14904938271604937,\n",
       " 0.11108818342151676,\n",
       " 0.13723985890652557,\n",
       " 0.10333068783068783,\n",
       " 0.0730432098765432,\n",
       " 0.12587125220458553,\n",
       " 0.12808818342151673,\n",
       " 0.13793738977072312,\n",
       " 0.09680423280423281,\n",
       " 0.09597707231040564,\n",
       " 0.1357310405643739,\n",
       " 0.07951322751322751,\n",
       " 0.13232451499118167,\n",
       " 0.10030864197530866,\n",
       " 0.15449470899470896,\n",
       " 0.15291093474426806,\n",
       " 0.12265961199294534,\n",
       " 0.09894444444444445,\n",
       " 0.1336869488536155,\n",
       " 0.1434567901234568,\n",
       " 0.10696296296296294,\n",
       " 0.08121075837742504,\n",
       " 0.10999559082892414,\n",
       " 0.11929805996472664]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtest(gt , target ,labels) : \n",
    "    print(gt)\n",
    "    print(target)\n",
    "    print(target[0].numpy())\n",
    "    print(labels[target[0].numpy()])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP_at_k(Fvec, imgLab,gt,rank=1):\n",
    "    # imgLab numpy.ndarray of shape: (8041,)\n",
    "   \n",
    "    N = len(imgLab) #8041 labels\n",
    "\n",
    "    imgLab = torch.LongTensor([imgLab[i] for i in range(len(imgLab))])\n",
    "    # imgLab.shape: [8041]\n",
    "    # Fvec.shape: [8041, 128]\n",
    "    \n",
    "    D = Fvec.mm(torch.t(Fvec)) # mm: matrix multiplication. (n×m) mm (m×p) results in  (n×p) tensor.\n",
    "    # [8041, 128] mm [128, 8041] --> [8041, 8041] this is D matrix\n",
    "    # There are 1's along the diagonal!\n",
    "    \n",
    "    D[torch.eye(len(imgLab)).bool()] = -1 \n",
    "    # torch.eye: Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "    # D[torch.eye(len(imgLab)).bool()]: diagonal elements of D will take a value of -1 ; the rest will remain the same\n",
    "    \n",
    "    #print(\"Distance Matris Shape\" , D.shape)\n",
    "\n",
    "    _,idx = D.topk(rank[-1])\n",
    "\n",
    "    mtest(y_true,idx,labels)\n",
    "    return mean_average_precision(np.array(gt),idx.numpy(), k= rank[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8], [11, 12, 13, 14, 15, 16, 17, 18, 19], [10, 12, 13, 14, 15, 16, 17, 18, 19], [10, 11, 13, 14, 15, 16, 17, 18, 19], [10, 11, 12, 14, 15, 16, 17, 18, 19], [10, 11, 12, 13, 15, 16, 17, 18, 19], [10, 11, 12, 13, 14, 16, 17, 18, 19], [10, 11, 12, 13, 14, 15, 17, 18, 19], [10, 11, 12, 13, 14, 15, 16, 18, 19], [10, 11, 12, 13, 14, 15, 16, 17, 19], [10, 11, 12, 13, 14, 15, 16, 17, 18], [21, 22, 23, 24, 25, 26, 27, 28, 29], [20, 22, 23, 24, 25, 26, 27, 28, 29], [20, 21, 23, 24, 25, 26, 27, 28, 29], [20, 21, 22, 24, 25, 26, 27, 28, 29], [20, 21, 22, 23, 25, 26, 27, 28, 29], [20, 21, 22, 23, 24, 26, 27, 28, 29], [20, 21, 22, 23, 24, 25, 27, 28, 29], [20, 21, 22, 23, 24, 25, 26, 28, 29], [20, 21, 22, 23, 24, 25, 26, 27, 29], [20, 21, 22, 23, 24, 25, 26, 27, 28], [31, 32, 33, 34, 35, 36, 37, 38, 39], [30, 32, 33, 34, 35, 36, 37, 38, 39], [30, 31, 33, 34, 35, 36, 37, 38, 39], [30, 31, 32, 34, 35, 36, 37, 38, 39], [30, 31, 32, 33, 35, 36, 37, 38, 39], [30, 31, 32, 33, 34, 36, 37, 38, 39], [30, 31, 32, 33, 34, 35, 37, 38, 39], [30, 31, 32, 33, 34, 35, 36, 38, 39], [30, 31, 32, 33, 34, 35, 36, 37, 39], [30, 31, 32, 33, 34, 35, 36, 37, 38], [41, 42, 43, 44, 45, 46, 47, 48, 49], [40, 42, 43, 44, 45, 46, 47, 48, 49], [40, 41, 43, 44, 45, 46, 47, 48, 49], [40, 41, 42, 44, 45, 46, 47, 48, 49], [40, 41, 42, 43, 45, 46, 47, 48, 49], [40, 41, 42, 43, 44, 46, 47, 48, 49], [40, 41, 42, 43, 44, 45, 47, 48, 49], [40, 41, 42, 43, 44, 45, 46, 48, 49], [40, 41, 42, 43, 44, 45, 46, 47, 49], [40, 41, 42, 43, 44, 45, 46, 47, 48]]\n",
      "tensor([[0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0019, 0.0011,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0031, 0.0024,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "[0.00153441 0.         0.         ... 0.         0.         0.        ]\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "mtest(y_true,embeddings,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8], [11, 12, 13, 14, 15, 16, 17, 18, 19], [10, 12, 13, 14, 15, 16, 17, 18, 19], [10, 11, 13, 14, 15, 16, 17, 18, 19], [10, 11, 12, 14, 15, 16, 17, 18, 19], [10, 11, 12, 13, 15, 16, 17, 18, 19], [10, 11, 12, 13, 14, 16, 17, 18, 19], [10, 11, 12, 13, 14, 15, 17, 18, 19], [10, 11, 12, 13, 14, 15, 16, 18, 19], [10, 11, 12, 13, 14, 15, 16, 17, 19], [10, 11, 12, 13, 14, 15, 16, 17, 18], [21, 22, 23, 24, 25, 26, 27, 28, 29], [20, 22, 23, 24, 25, 26, 27, 28, 29], [20, 21, 23, 24, 25, 26, 27, 28, 29], [20, 21, 22, 24, 25, 26, 27, 28, 29], [20, 21, 22, 23, 25, 26, 27, 28, 29], [20, 21, 22, 23, 24, 26, 27, 28, 29], [20, 21, 22, 23, 24, 25, 27, 28, 29], [20, 21, 22, 23, 24, 25, 26, 28, 29], [20, 21, 22, 23, 24, 25, 26, 27, 29], [20, 21, 22, 23, 24, 25, 26, 27, 28], [31, 32, 33, 34, 35, 36, 37, 38, 39], [30, 32, 33, 34, 35, 36, 37, 38, 39], [30, 31, 33, 34, 35, 36, 37, 38, 39], [30, 31, 32, 34, 35, 36, 37, 38, 39], [30, 31, 32, 33, 35, 36, 37, 38, 39], [30, 31, 32, 33, 34, 36, 37, 38, 39], [30, 31, 32, 33, 34, 35, 37, 38, 39], [30, 31, 32, 33, 34, 35, 36, 38, 39], [30, 31, 32, 33, 34, 35, 36, 37, 39], [30, 31, 32, 33, 34, 35, 36, 37, 38], [41, 42, 43, 44, 45, 46, 47, 48, 49], [40, 42, 43, 44, 45, 46, 47, 48, 49], [40, 41, 43, 44, 45, 46, 47, 48, 49], [40, 41, 42, 44, 45, 46, 47, 48, 49], [40, 41, 42, 43, 45, 46, 47, 48, 49], [40, 41, 42, 43, 44, 46, 47, 48, 49], [40, 41, 42, 43, 44, 45, 47, 48, 49], [40, 41, 42, 43, 44, 45, 46, 48, 49], [40, 41, 42, 43, 44, 45, 46, 47, 49], [40, 41, 42, 43, 44, 45, 46, 47, 48]]\n",
      "tensor([[ 2, 32,  6, 34,  5, 17, 27,  9,  8],\n",
      "        [47, 25,  4, 38, 22, 42, 30, 28, 44],\n",
      "        [32, 17, 34,  5,  0,  6, 27,  9, 31],\n",
      "        [32, 34,  9,  2, 17, 14, 35,  0, 40],\n",
      "        [17, 14, 32, 34,  2,  5, 31, 30, 25],\n",
      "        [ 2, 32, 17, 34,  0,  6, 27, 20,  9],\n",
      "        [ 0,  2,  5,  7, 32, 27, 34, 17, 31],\n",
      "        [ 6, 41,  8,  0, 27,  2,  9,  5, 31],\n",
      "        [ 6,  0,  9,  7, 24,  2,  5, 27, 32],\n",
      "        [ 2, 32,  0, 34,  5, 17,  6, 15, 24],\n",
      "        [46, 21, 41, 31,  6,  7, 49, 39,  5],\n",
      "        [ 9,  4, 15, 38, 19, 14, 17, 32, 47],\n",
      "        [30, 38, 14, 47, 42, 48, 35, 25,  4],\n",
      "        [20, 16, 35, 24, 37, 48, 17,  5, 36],\n",
      "        [35, 17,  4, 34, 32, 38,  2, 30,  5],\n",
      "        [ 9,  2, 32, 19, 34,  0,  8, 27, 17],\n",
      "        [20, 33, 13, 17, 37, 36, 24,  5, 35],\n",
      "        [ 2, 32, 34,  5,  0, 14, 35,  4, 27],\n",
      "        [26, 35, 33, 34,  0, 36, 23, 20,  2],\n",
      "        [17,  5, 32,  2,  9,  4, 15, 34, 24],\n",
      "        [ 5, 17, 35, 34, 32,  2, 36, 14,  0],\n",
      "        [46, 31,  6, 27, 41,  7, 10,  2,  5],\n",
      "        [25, 17, 30, 14,  4, 38, 47, 33, 44],\n",
      "        [34,  2, 32, 17,  5, 44,  0, 27, 24],\n",
      "        [ 5,  2,  9, 32, 17, 23,  8,  0, 34],\n",
      "        [14,  4, 38, 30, 44, 17, 47, 34, 32],\n",
      "        [35, 34, 23, 17,  0,  2, 32, 27, 14],\n",
      "        [ 2, 34, 32,  5,  6,  0, 17, 31, 23],\n",
      "        [46, 21, 41, 27, 44, 31, 23,  2,  6],\n",
      "        [ 7,  6,  8, 41, 49, 27,  0, 24,  9],\n",
      "        [14,  4, 38, 25, 47, 17, 42, 35, 34],\n",
      "        [27,  2,  6, 32, 34,  5, 17,  9,  0],\n",
      "        [ 2, 34, 17,  5,  0,  6, 27,  9,  4],\n",
      "        [24, 17, 36,  9, 34,  5,  2, 32, 23],\n",
      "        [32,  2, 17,  5,  0, 36, 27,  6, 35],\n",
      "        [14, 17, 34, 32, 20,  2, 26,  5, 36],\n",
      "        [34, 17, 32,  2,  0,  5,  9, 35, 20],\n",
      "        [ 5, 24, 20, 17, 19, 23, 32,  9,  2],\n",
      "        [14, 30,  4, 25, 35, 48, 17, 47, 19],\n",
      "        [10, 37, 46, 49,  6, 41,  7, 31,  5],\n",
      "        [ 3, 34,  5, 17, 32,  2, 14, 23,  9],\n",
      "        [ 7,  6, 46, 49, 21, 27,  8, 31, 10],\n",
      "        [30, 47, 38,  4, 25, 14, 17,  3, 19],\n",
      "        [21, 31, 46,  7, 15,  9, 23, 44, 27],\n",
      "        [23, 34, 32, 17,  2, 31, 27,  5, 36],\n",
      "        [46, 41, 27, 44, 31, 15, 49, 10, 21],\n",
      "        [21,  6, 31, 41, 27,  7,  2,  5, 10],\n",
      "        [ 4, 30, 19, 25, 38, 14, 17, 32, 15],\n",
      "        [35, 14, 38, 34, 17, 20, 26, 19, 23],\n",
      "        [41,  8,  7,  6, 46, 27, 24,  0,  5]])\n",
      "[ 2 32  6 34  5 17 27  9  8]\n",
      "tensor([0, 3, 0, 3, 0, 1, 2, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11929805996472664"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mAP_at_k(embeddings,labels,y_true,[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GT = [1,2,3,4,5,6,7,8,9] # ilk eleman için\n",
    "pred0 = [2,32,6,34,5,17,27,9,8]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n",
      "0.5555555555555555\n",
      "0.41666666666666663\n",
      "0.4533333333333333\n",
      "0.37777777777777777\n",
      "0.3238095238095238\n",
      "0.3458333333333333\n",
      "0.3691358024691358\n"
     ]
    }
   ],
   "source": [
    "print(average_precision_at_k(GT,pred0,k=1))\n",
    "print(average_precision_at_k(GT,pred0,k=2))\n",
    "print(average_precision_at_k(GT,pred0,k=3))\n",
    "print(average_precision_at_k(GT,pred0,k=4))\n",
    "print(average_precision_at_k(GT,pred0,k=5))\n",
    "print(average_precision_at_k(GT,pred0,k=6))\n",
    "print(average_precision_at_k(GT,pred0,k=7))\n",
    "print(average_precision_at_k(GT,pred0,k=8))\n",
    "print(average_precision_at_k(GT,pred0,k=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1\n",
      "0.5 0\n",
      "0.6666666666666666 1\n",
      "0.5 0\n",
      "0.6 1\n",
      "0.5 0\n",
      "0.42857142857142855 0\n",
      "0.5 1\n",
      "0.5555555555555556 1\n"
     ]
    }
   ],
   "source": [
    "GT = [1,2,3,4,5,6,7,8,9] # ilk eleman için\n",
    "pred0 = [2,32,6,34,5,17,27,9,8]\n",
    "print(precision_at_k(GT,pred0,k=1) , rel_at_k(GT,pred0,k=1))\n",
    "print(precision_at_k(GT,pred0,k=2), rel_at_k(GT,pred0,k=2))\n",
    "print(precision_at_k(GT,pred0,k=3), rel_at_k(GT,pred0,k=3))\n",
    "print(precision_at_k(GT,pred0,k=4), rel_at_k(GT,pred0,k=4))\n",
    "print(precision_at_k(GT,pred0,k=5), rel_at_k(GT,pred0,k=5))\n",
    "print(precision_at_k(GT,pred0,k=6), rel_at_k(GT,pred0,k=6))\n",
    "print(precision_at_k(GT,pred0,k=7), rel_at_k(GT,pred0,k=7))\n",
    "print(precision_at_k(GT,pred0,k=8), rel_at_k(GT,pred0,k=8))\n",
    "print(precision_at_k(GT,pred0,k=9), rel_at_k(GT,pred0,k=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166665"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.666666 / 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 3, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 4, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 5, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 6, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 7, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 8, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 9],\n",
       " [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       " [11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [10, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [10, 11, 13, 14, 15, 16, 17, 18, 19],\n",
       " [10, 11, 12, 14, 15, 16, 17, 18, 19],\n",
       " [10, 11, 12, 13, 15, 16, 17, 18, 19],\n",
       " [10, 11, 12, 13, 14, 16, 17, 18, 19],\n",
       " [10, 11, 12, 13, 14, 15, 17, 18, 19],\n",
       " [10, 11, 12, 13, 14, 15, 16, 18, 19],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 19],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
       " [21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       " [20, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       " [20, 21, 23, 24, 25, 26, 27, 28, 29],\n",
       " [20, 21, 22, 24, 25, 26, 27, 28, 29],\n",
       " [20, 21, 22, 23, 25, 26, 27, 28, 29],\n",
       " [20, 21, 22, 23, 24, 26, 27, 28, 29],\n",
       " [20, 21, 22, 23, 24, 25, 27, 28, 29],\n",
       " [20, 21, 22, 23, 24, 25, 26, 28, 29],\n",
       " [20, 21, 22, 23, 24, 25, 26, 27, 29],\n",
       " [20, 21, 22, 23, 24, 25, 26, 27, 28],\n",
       " [31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       " [30, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       " [30, 31, 33, 34, 35, 36, 37, 38, 39],\n",
       " [30, 31, 32, 34, 35, 36, 37, 38, 39],\n",
       " [30, 31, 32, 33, 35, 36, 37, 38, 39],\n",
       " [30, 31, 32, 33, 34, 36, 37, 38, 39],\n",
       " [30, 31, 32, 33, 34, 35, 37, 38, 39],\n",
       " [30, 31, 32, 33, 34, 35, 36, 38, 39],\n",
       " [30, 31, 32, 33, 34, 35, 36, 37, 39],\n",
       " [30, 31, 32, 33, 34, 35, 36, 37, 38],\n",
       " [41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       " [40, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       " [40, 41, 43, 44, 45, 46, 47, 48, 49],\n",
       " [40, 41, 42, 44, 45, 46, 47, 48, 49],\n",
       " [40, 41, 42, 43, 45, 46, 47, 48, 49],\n",
       " [40, 41, 42, 43, 44, 46, 47, 48, 49],\n",
       " [40, 41, 42, 43, 44, 45, 47, 48, 49],\n",
       " [40, 41, 42, 43, 44, 45, 46, 48, 49],\n",
       " [40, 41, 42, 43, 44, 45, 46, 47, 49],\n",
       " [40, 41, 42, 43, 44, 45, 46, 47, 48]]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample =  triplet_imagenet_dataset.sample(\"test\",mode=\"img_retrieval\",samples_per_class = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_query = test_sample[2]\n",
    "y_query = test_sample[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_318846/1144268499.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  np.array(x_query).shape\n",
      "/tmp/ipykernel_318846/1144268499.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(x_query).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_query).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import RetrievalMAP\n",
    "\n",
    "indexes = \n",
    "\n",
    "\n",
    "target = [rel_at_k() for i in ra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37bb63493fa6dddf35bfed9ad35536dbf43dc1c10eb467e9be32956dae9593c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('osmansamed')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
