{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas.core.common import flatten\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import learn2learn as l2l\n",
    "from losses import *\n",
    "from Triplets import *\n",
    "from cnn4_triplet import *\n",
    "\n",
    "import datasets\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triplet_Model_Parameter = {\n",
    "    \"CIFARFS\" : {\"data\" : TripletFSCIFAR100 , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":256,\"margin\":1.0,\"lambda\":1} ,\n",
    "    \"CUB\" : {\"data\" : TripletCUB , \"root\" : \"./data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "    \"FLOWERS\" : {\"data\" : TripletFlowers , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "    \"MINIIMAGENET\" : {\"data\" : TripletMiniImageNet , \"root\" : \"data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":32, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":800,\"margin\":1.0,\"lambda\":1},\n",
    "    \"OMNIGLOT\" : {\"data\" : TripletOmniglot , \"root\" : \"~/data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor(),transforms.Resize((28,28))]), \"hidden_size\":64, \"layers\":4, \"channels\":1, \"max_pool\":False, \"embedding_size\":256,\"margin\":1.0,\"lambda\":1},\n",
    "    \"MINIIMAGENET_64\" : {\"data\" : TripletMiniImageNet , \"root\" : \"data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":64, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":1600,\"margin\":1.0,\"lambda\":1},\n",
    "    \"MINIIMAGENET_RS\" : {\"data\" : TripletMiniImageNet , \"root\" : \"data\", \"download\": True , \"transform\" : transforms.Compose([transforms.ToTensor()]), \"hidden_size\":128, \"layers\":4, \"channels\":3, \"max_pool\":True, \"embedding_size\":3200,\"margin\":1.0,\"lambda\":1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets, shots):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    # return (predictions == targets).sum().float() / targets.size(0)\n",
    "\n",
    "    if shots == 1:\n",
    "        mask = np.array([True, False, False, False, False, False, False, False, True, True, True, True])\n",
    "    else: # shots==5\n",
    "        mask = np.array(\n",
    "        [True, False, False, False, False, False, False, False, False, False, False, False,\tFalse, False, False, False, False, False, False, False, \n",
    "        True, False, False, False, False, True, False, False, False, False, True, False, False, False, False, True, False, False, False, False,\n",
    "        True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True ,True, True])\n",
    "    return (predictions[mask] == targets[mask]).sum().float() / targets[mask].size(0)\n",
    "\n",
    "\n",
    "def fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    adaptation_data, adaptation_labels, evaluation_data, evaluation_labels = batch\n",
    "    adaptation_data[0]= adaptation_data[0].to(device)\n",
    "    adaptation_data[1]= adaptation_data[1].to(device)\n",
    "    adaptation_data[2]= adaptation_data[2].to(device)\n",
    "\n",
    "    adaptation_labels= adaptation_labels.to(device)\n",
    "\n",
    "    evaluation_data[0]= evaluation_data[0].to(device)\n",
    "    evaluation_data[1]= evaluation_data[1].to(device)\n",
    "    evaluation_data[2]= evaluation_data[2].to(device)\n",
    "\n",
    "    evaluation_labels= evaluation_labels.to(device)\n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        out= learner(adaptation_data[0], adaptation_data[1], adaptation_data[2] )\n",
    "        train_error = loss(out[0],out[1],out[2], torch.vstack([out[3],out[4],out[5]]), torch.hstack([adaptation_labels[0],adaptation_labels[1],adaptation_labels[2]]))\n",
    "        learner.adapt(train_error, allow_unused=True)\n",
    "\n",
    "    # Evaluate the adapted model\n",
    "    predictions = learner(evaluation_data[0], evaluation_data[1], evaluation_data[2])\n",
    "    valid_error= loss(predictions[0],predictions[1],predictions[2], torch.vstack([predictions[3],predictions[4],predictions[5]]), torch.hstack([evaluation_labels[0],evaluation_labels[1],evaluation_labels[2]]))\n",
    "    valid_accuracy = accuracy(torch.vstack([predictions[3],predictions[4],predictions[5]]), torch.hstack([evaluation_labels[0],evaluation_labels[1],evaluation_labels[2]]), shots)\n",
    "    return valid_error, valid_accuracy\n",
    "\n",
    "def fast_adapt_image_retrieval(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    adaptation_data, adaptation_labels, evaluation_data, evaluation_labels = batch\n",
    "    adaptation_data[0]= adaptation_data[0].to(device)\n",
    "    adaptation_data[1]= adaptation_data[1].to(device)\n",
    "    adaptation_data[2]= adaptation_data[2].to(device)\n",
    "\n",
    "    adaptation_labels= adaptation_labels.to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        out= learner(adaptation_data[0], adaptation_data[1], adaptation_data[2] )\n",
    "        train_error = loss(out[0],out[1],out[2], torch.vstack([out[3],out[4],out[5]]), torch.hstack([adaptation_labels[0],adaptation_labels[1],adaptation_labels[2]]))\n",
    "        learner.adapt(train_error, allow_unused=True)\n",
    "\n",
    "   \n",
    "    return train_error, train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways=5  # in our triplet implementation, number of distinct classes is 5\n",
    "shots=1\n",
    "meta_lr=0.001  # as in MAML\n",
    "fast_lr=0.01  # as in MAML\n",
    "meta_batch_size=4  # Maml Omniglot:32; miniImageNet: 4\n",
    "adaptation_steps=5\n",
    "test_adaptation_steps=10\n",
    "num_iterations=60000  # as in MAML\n",
    "cuda=True\n",
    "seed=42\n",
    "num_test_episodes=600\n",
    "selected_model=\"MINIIMAGENET_64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device('cuda')    \n",
    "\n",
    "triplet_imagenet_dataset = Triplet_Model_Parameter[selected_model][\"data\"](root = Triplet_Model_Parameter[selected_model][\"root\"], download = Triplet_Model_Parameter[selected_model][\"download\"], transform = Triplet_Model_Parameter[selected_model][\"transform\"])\n",
    "\n",
    "# Create model using saved parameters:\n",
    "model = TripletCNN4(output_size= ways, hidden_size=Triplet_Model_Parameter[selected_model][\"hidden_size\"], layers=Triplet_Model_Parameter[selected_model][\"layers\"], channels=Triplet_Model_Parameter[selected_model][\"channels\"], max_pool=Triplet_Model_Parameter[selected_model][\"max_pool\"], embedding_size=Triplet_Model_Parameter[selected_model][\"embedding_size\"])\n",
    "\n",
    "try : \n",
    "    model = torch.load(\"./Tripletmaml_MINIIMAGENET_RetrievalTest_batchsize4_shots1.pt\")\n",
    "    model.to(device)\n",
    "except : \n",
    "    model = TripletCNN4(output_size= ways, hidden_size=Triplet_Model_Parameter[selected_model][\"hidden_size\"], layers=Triplet_Model_Parameter[selected_model][\"layers\"], channels=Triplet_Model_Parameter[selected_model][\"channels\"], max_pool=Triplet_Model_Parameter[selected_model][\"max_pool\"], embedding_size=Triplet_Model_Parameter[selected_model][\"embedding_size\"])\n",
    "    model.load_state_dict(torch.load(\"./Tripletmaml_MINIIMAGENET_RetrievalTest_batchsize4_shots1.pt\"))\n",
    "    model.to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(maml.parameters(), meta_lr) # meta-update\n",
    "triplet_w= Triplet_Model_Parameter[selected_model][\"lambda\"]\n",
    "combined_loss_fn= CombinedLoss2(triplet_w, shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Precision at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Precision at k\n",
    "    \"\"\"\n",
    "    intersection = np.intersect1d(y_true, y_pred[:k])\n",
    "    return len(intersection) / k\n",
    "\n",
    "def rel_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Relevance at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Relevance at k\n",
    "    \"\"\"\n",
    "    if y_pred[k-1] in y_true:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def average_precision_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes Average Precision at k for one sample\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           Average Precision at k\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    rel_counter = 0\n",
    "    for i in range(1, k+1):\n",
    "        ap += precision_at_k(y_true, y_pred, i) * rel_at_k(y_true, y_pred, i)\n",
    "        rel_counter += rel_at_k(y_true, y_pred, i)\n",
    "    #return ap / min(k, len(y_true))\n",
    "    if rel_counter == 0:\n",
    "        return  0\n",
    "    return ap / rel_counter\n",
    "\n",
    "\n",
    "def mean_average_precision(y_true, y_pred, k=12):\n",
    "    \"\"\" Computes MAP at k\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    y_true: np.array\n",
    "            2D Array of correct recommendations (Order doesn't matter)\n",
    "    y_pred: np.array\n",
    "            2D Array of predicted recommendations (Order does matter)\n",
    "    k: int, optional\n",
    "       Maximum number of predicted recommendations\n",
    "            \n",
    "    Returns\n",
    "    _______\n",
    "    score: double\n",
    "           MAP at k\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean([average_precision_at_k(gt, pred, k) \\\n",
    "                    for gt, pred in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP_at_k(Fvec, imgLab,gt,rank=1,posonly=False):\n",
    "    # imgLab numpy.ndarray of shape: (8041,)\n",
    "   \n",
    "    N = len(imgLab) #8041 labels\n",
    "\n",
    "    imgLab = torch.LongTensor([imgLab[i] for i in range(len(imgLab))])\n",
    "    # imgLab.shape: [8041]\n",
    "    # Fvec.shape: [8041, 128]\n",
    "    \n",
    "    D = Fvec.mm(torch.t(Fvec)) # mm: matrix multiplication. (n×m) mm (m×p) results in  (n×p) tensor.\n",
    "    # [8041, 128] mm [128, 8041] --> [8041, 8041] this is D matrix\n",
    "    # There are 1's along the diagonal!\n",
    "    \n",
    "    D[torch.eye(len(imgLab)).bool()] = -1 \n",
    "    # torch.eye: Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "    # D[torch.eye(len(imgLab)).bool()]: diagonal elements of D will take a value of -1 ; the rest will remain the same\n",
    "    \n",
    "    #print(\"Distance Matris Shape\" , D.shape)\n",
    "    #print(D)\n",
    "    _,idx = D.topk(rank[-1])\n",
    "\n",
    "    preds = np.array([imgLab[i].numpy() for i in idx])\n",
    "\n",
    "    if posonly == \"True\":\n",
    "        return mean_average_precision(gt[:10],preds[:10], k= rank[-1]),idx\n",
    "    elif posonly == \"divide_by_class\" : \n",
    "        return [mean_average_precision(gt[:10],preds[:10], k= rank[-1]),mean_average_precision(gt[10:20],preds[10:20], k= rank[-1]),mean_average_precision(gt[20:30],preds[20:30], k= rank[-1]),mean_average_precision(gt[30:40],preds[30:40], k= rank[-1]),mean_average_precision(gt[40:50],preds[40:50], k= rank[-1]),mean_average_precision(gt,preds, k= rank[-1])],idx\n",
    "    else :\n",
    "        return mean_average_precision(gt,preds, k= rank[-1]),idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ground_truth(labels , k ):\n",
    "    empty_array =    np.empty((len(labels),k),dtype= np.int32)\n",
    "    for idx, label in enumerate(labels) : \n",
    "        empty_array[idx] = np.repeat(label,k)\n",
    "    \n",
    "    return empty_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "       [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "       [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ground_truth([1,2,3,4,5,6],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_support(batch,save_as):\n",
    "        img_batch_vis = np.array((batch[0][0].numpy(),batch[0][1].numpy(),batch[0][2].numpy())).transpose(1,0,2,3,4)\n",
    "\n",
    "        lbl_batch_vis = np.stack((batch[1][0].numpy(),batch[1][1].numpy(),batch[1][2].numpy())).transpose((1,0))\n",
    "\n",
    "\n",
    "        # Create a figure with 4 rows and 3 columns\n",
    "        fig, axs = plt.subplots(4, 3)\n",
    "        fig.subplots_adjust(hspace=1, wspace=0.5)\n",
    "        fig.set_facecolor('white')\n",
    "        fig.suptitle(\"Support\", fontsize=16)\n",
    "        # Plot each element in the array as an image in the figure\n",
    "        for i in range(4):\n",
    "                for j in range(3):\n",
    "                        axs[i,j].set_title(lbl_batch_vis[i,j])\n",
    "                        axs[i,j].set_xticks([])\n",
    "                        axs[i,j].set_yticks([])\n",
    "                        axs[i,j].imshow(torch.from_numpy(img_batch_vis[i,j]).permute(1, 2, 0).cpu().int())\n",
    "        fig.savefig(save_as)\n",
    "\n",
    "def draw_query(batch,save_as,sample_per_class):\n",
    "        Query = np.array([i.int().numpy() for i in batch[2]])\n",
    "        Query = Query.reshape(5,sample_per_class,3,84,84)\n",
    "        Query.shape\n",
    "\n",
    "        query_label = np.array(batch[3])\n",
    "        query_label = query_label.reshape(5,sample_per_class)\n",
    "\n",
    "        # Create a figure with 5 rows and 10 columns\n",
    "        fig, axs = plt.subplots(5, sample_per_class)\n",
    "        fig.subplots_adjust(hspace=1, wspace=0.5)\n",
    "        fig.set_facecolor('white')\n",
    "        fig.suptitle(\"Query Pool\", fontsize=16)\n",
    "        # Plot each element in the array as an image in the figure\n",
    "        for i in range(5):\n",
    "                for j in range(sample_per_class):\n",
    "                        axs[i,j].set_title(query_label[i,j])\n",
    "                        axs[i,j].set_xticks([])\n",
    "                        axs[i,j].set_yticks([])\n",
    "                        axs[i,j].imshow(torch.from_numpy(Query[i,j]).permute(1, 2, 0).cpu().int())\n",
    "        fig.savefig(save_as)\n",
    "\n",
    "def draw_preds(batch,pred_ids,save_as,k):\n",
    "        Query = np.array([i.int().numpy() for i in batch[2]])\n",
    "\n",
    "        query_label = np.array(batch[3])\n",
    "        \n",
    "        fig, axs = plt.subplots(Query.shape[0], k+1, figsize=(25, 50))\n",
    "        fig.subplots_adjust(hspace=1, wspace=0.5)\n",
    "        fig.set_facecolor('white')\n",
    "        fig.suptitle(\"Predictions\", fontsize=16)\n",
    "        for i in range(Query.shape[0]):\n",
    "                for j in range(k+1):\n",
    "                        if j == 0 :\n",
    "                                axs[i,j].set_title(query_label[i])\n",
    "                                axs[i,j].set_xticks([])\n",
    "                                axs[i,j].set_yticks([])\n",
    "                                axs[i,j].imshow(torch.from_numpy(Query[i]).permute(1, 2, 0).cpu().int())\n",
    "                        else : \n",
    "                                axs[i,j].set_title(query_label[pred_ids[i][j-1]])\n",
    "                                axs[i,j].set_xticks([])\n",
    "                                axs[i,j].set_yticks([])\n",
    "                                axs[i,j].imshow(torch.from_numpy(Query[pred_ids[i][j-1]]).permute(1, 2, 0).cpu().int())\n",
    "\n",
    "        fig.savefig(save_as)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'triplet_imagenet_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73068/536945998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iteration\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplet_imagenet_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"img_retrieval\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for each batch, gradients should be cleaned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# Compute meta-training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'triplet_imagenet_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for testmap in [1,5,9]:\n",
    "    mean_map = []\n",
    "    with open(\"64_imagenet_all_10iter_visuzalization_labelization_MAP\"+str(testmap)+\".txt\",\"w\") as file :\n",
    "        for i in range(10):\n",
    "            print(\"iteration\" ,i)\n",
    "            a = triplet_imagenet_dataset.sample(\"test\",mode=\"img_retrieval\",samples_per_class = 10)\n",
    "            opt.zero_grad() # for each batch, gradients should be cleaned.\n",
    "            # Compute meta-training loss\n",
    "            # print('Task no: ', task)\n",
    "            learner = maml.clone()\n",
    "            batch = a\n",
    "            draw_support(batch,\"./visualization/\"+\"64_imagenet_all_10iter_visuzalization_labelization_sup_MAP\"+str(testmap)+\"_\"+str(i)+\".png\")\n",
    "            draw_query(batch,\"./visualization/\"+\"64_imagenet_all_10iter_visuzalization_labelization_query_MAP\"+str(testmap)+\"_\"+str(i)+\".png\",sample_per_class=10)\n",
    "            train_error, _ = fast_adapt_image_retrieval(batch,\n",
    "                                                                learner,\n",
    "                                                                combined_loss_fn,\n",
    "                                                                adaptation_steps,\n",
    "                                                                shots,\n",
    "                                                                ways,\n",
    "                                                                device)                                                \n",
    "            embeddings = []\n",
    "            labels = torch.from_numpy(np.array(list(a[3])))\n",
    "            for idx,image in enumerate(a[2]):\n",
    "                embedding,class_prob =learner.forward_once(torch.unsqueeze(image,0).to(device)) \n",
    "                embedding = F.normalize(embedding, p = 2 , dim =1).cpu()\n",
    "                embeddings.append(np.array(embedding[0].tolist()))\n",
    "            embeddings = torch.from_numpy(np.array(embeddings))\n",
    "\n",
    "            map,pred_ids = mAP_at_k(embeddings,labels,generate_ground_truth(labels,testmap),[testmap],posonly=\"divide_by_class\")\n",
    "            draw_preds(batch,pred_ids,\"./visualization/\"+\"64_imagenet_all_10iter_visuzalization_labelization_preds_MAP\"+str(testmap)+\"_\"+str(i)+\".png\",testmap)\n",
    "            file.write(\"iteration - \" + str(i) + \" mAP@\"+str(testmap)+\" :\" + str(map) + \"\\n\" )\n",
    "            mean_map.append(map)\n",
    "        print(np.mean(mean_map))\n",
    "        file.write(\"General Mean - \" + str(i) + \" mAP@\"+str(testmap)+\"  :\" + str(np.mean(mean_map)) + \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 19:58:26) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c746eb95852000f7e61dfc0add9c8194ac091bc77cc920f3c9e782bc8f4568c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
